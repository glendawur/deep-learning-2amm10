{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d32f8d18"
   },
   "source": [
    "# Group Number:\n",
    "\n",
    "# Student 1: Ryan Meghoe\n",
    "\n",
    "# Student 2: Nikita Jain\n",
    "\n",
    "# Student 3: Andrei Rykov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting notes about implementation of the VAE with convolutional layers:\n",
    "\n",
    "https://towardsdatascience.com/building-a-convolutional-vae-in-pytorch-a0f54c947f71\n",
    "https://debuggercafe.com/convolutional-variational-autoencoder-in-pytorch-on-mnist-dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7iMjOIffegC"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MwKglIGfegD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "# other imports go here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmNxFthzfegE"
   },
   "source": [
    "# Data loading and inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0owj4HufegE"
   },
   "outputs": [],
   "source": [
    "# load and inspect data\n",
    "data_location = 'https://surfdrive.surf.nl/files/index.php/s/K3ArFDQJb5USQ6K/download'\n",
    "data_request = requests.get(data_location)\n",
    "full_data = pickle.loads(data_request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "data_location = 'https://surfdrive.surf.nl/files/index.php/s/K3ArFDQJb5USQ6K/download'\n",
    "data_request = requests.get(data_location)\n",
    "full_data = pickle.loads(data_request.content)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "class FashionMNISTDataset(Dataset):\n",
    "    def __init__(self, data, targets = None, transform=None):\n",
    "        self.data = torch.Tensor(data)\n",
    "\n",
    "        if targets is None:\n",
    "            self.targets = torch.Tensor(torch.zeros(data.shape[0], 1))\n",
    "        else:\n",
    "            assert data.shape[0] == targets.shape[0]\n",
    "            self.targets = torch.Tensor(targets)\n",
    "\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            x = self.transform(self.data[index])\n",
    "        else:\n",
    "            x = self.data[index]\n",
    "        return x, self.targets[index]\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "augmentation = transforms.RandomAffine(degrees = (-20, 20), translate=(0.1, 0.1), scale=(0.8, 1.1), fill  = 0)\n",
    "\n",
    "unlabeled_dataset = FashionMNISTDataset(torch.Tensor(full_data['unlabeled_data']), transform=augmentation)\n",
    "\n",
    "classification_dataset = FashionMNISTDataset(torch.Tensor(full_data['labeled_data']['data']),\n",
    "                                             torch.Tensor(full_data['labeled_data']['labels']),\n",
    "                                             transform=augmentation)\n",
    "\n",
    "anomaly1_dataset = TensorDataset(torch.Tensor(full_data['representative_set_1']['data']),\n",
    "                                       torch.Tensor(full_data['representative_set_1']['labels']))\n",
    "                                       \n",
    "anomaly2_dataset = TensorDataset(torch.Tensor(full_data['representative_set_2']['data']),\n",
    "                                       torch.Tensor(full_data['representative_set_2']['labels']))\n",
    "                                       \n",
    "                                       \n",
    "train_dataloader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True)\n",
    "classification_dataloader = DataLoader(classification_dataset, batch_size=batch_size, shuffle=False)\n",
    "anomaly1_dataloader = DataLoader(anomaly1_dataset, batch_size=batch_size, shuffle=False)\n",
    "anomaly2_dataloader = DataLoader(anomaly2_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g48a4EOZfegF"
   },
   "source": [
    "# Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ht_CmVw1fegH"
   },
   "outputs": [],
   "source": [
    "# code for model definitions goes here\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, stride=2, padding=1)  # 4x4x256\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn1d = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc1 = nn.Linear(4*4*256, 128)\n",
    "        self.fc_mu = nn.Linear(128, z_dim)\n",
    "        self.fc_var = nn.Linear(128, z_dim)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        x = self.bn1(F.leaky_relu(self.conv1(image)))\n",
    "        x = self.bn2(F.leaky_relu(self.conv2(x)))\n",
    "        x = self.bn3(F.leaky_relu(self.conv3(x)))\n",
    "        x = self.bn4(F.leaky_relu(self.conv4(x)))\n",
    "        x = self.bn1d(F.leaky_relu(self.fc1(x.view(-1, 4*4*256))))\n",
    "        mu = self.fc_mu(x)\n",
    "        var = self.fc_var(x)\n",
    "\n",
    "        return mu, var\n",
    "\n",
    "class Decoder(n.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.FC = nn.Linear(latent_dim, 2*2*256)\n",
    "        self.Conv1 = nn.ConvTranspose2d(256, 128, 3, stride=2, output_padding =1) # 6x6x128\n",
    "        self.Conv2 = nn.ConvTranspose2d(128, 64, 3, stride=2, output_padding =1) # 14x14x64\n",
    "        self.Conv3 = nn.ConvTranspose2d(64, 32, 3, stride=2, output_padding =1) # 30x30x32\n",
    "        self.Conv4 = nn.ConvTranspose2d(32, 1, 3) # 32x32x1\n",
    "\n",
    "\n",
    "     \n",
    "    def forward(self,latent_dim):\n",
    "        x = F.relu(self.FC(latent_dim))\n",
    "        # x = x.view(-1, 256*4*4)\n",
    "        x = x.view(-1, 256, 2, 2)\n",
    "        x = F.relu(self.Conv1(x))\n",
    "        x = F.relu(self.Conv2(x))\n",
    "        x = F.relu(self.Conv3(x))\n",
    "        # x = F.relu(self.Conv4(x))\n",
    "        x = torch.sigmoid(self.Conv4(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.enc = Encoder(z_dim)\n",
    "        self.dec = Decoder(z_dim)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mu + epsilon * std\n",
    "\n",
    "    def forward(self, input):\n",
    "        mu, log_var = self.enc(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        output = self.dec(z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIgBXhKNfegH"
   },
   "source": [
    "# Training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuTCVGqgfegI"
   },
   "outputs": [],
   "source": [
    "# write your training and validation loop here\n",
    "def loss_function(x, x_reconstr, mu, log_sigma):\n",
    "    reconstr_loss = nn.functional.mse_loss(x_reconstr, x, reduction='sum')\n",
    "    kl_loss = 0.5 * torch.sum(mu.pow(2) + (2*log_sigma).exp() - 2*log_sigma - 1)\n",
    "    total_loss = reconstr_loss + kl_loss\n",
    "    return total_loss, reconstr_loss, kl_loss\n",
    "\n",
    "def train_model(model, dataloader, loss, epochs: int = 50, device = torch.device('cpu')):\n",
    "    model.train()\n",
    "    overall_loss = 0\n",
    "    overall_reconstr_loss = 0\n",
    "    overall_kl_loss = 0\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (x, _) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_reconstr, mu, log_sigma = model(x)\n",
    "            loss, reconstr_loss, kl_loss = loss(x.view(x.shape[0], -1), x_reconstr.view(x.shape[0], -1), mu, log_sigma)\n",
    "            \n",
    "            overall_loss += loss.item()\n",
    "            overall_reconstr_loss += reconstr_loss.item()\n",
    "            overall_kl_loss += kl_loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        n_datapoints = batch_idx * batch_size\n",
    "    print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss / n_datapoints, \"\\tReconstruction Loss:\", overall_reconstr_loss / n_datapoints, \"\\tKL Loss:\", overall_kl_loss / n_datapoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B83YjnvIfegI"
   },
   "outputs": [],
   "source": [
    "# perform training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QOez5aPfegJ"
   },
   "source": [
    "# Inspection, Validation, and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhLFSFOkfegJ"
   },
   "outputs": [],
   "source": [
    "# Inspect, validate, and analyse your trained model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "assignment_4_skeleton.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "93aa0139fa57dda094a7ddd49a7a44ee21342a7ee37e3f7e1205953a77a5c532"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
