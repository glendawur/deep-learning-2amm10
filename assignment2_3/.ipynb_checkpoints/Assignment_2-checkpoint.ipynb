{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32f8d18",
   "metadata": {
    "id": "d32f8d18"
   },
   "source": [
    "# Group Number:\n",
    "\n",
    "# Student 1: Ryan Meghoe\n",
    "\n",
    "# Student 2: Nikita Jain\n",
    "\n",
    "# Student 3: Andrei Rykov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec2056",
   "metadata": {
    "id": "faec2056"
   },
   "source": [
    "# Downloading Data and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d0580a5",
   "metadata": {
    "id": "7d0580a5"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0756591",
   "metadata": {
    "id": "b0756591"
   },
   "outputs": [],
   "source": [
    "def load_zip(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipf = ZipFile(io.BytesIO(response.content))\n",
    "    return {name: zipf.read(name) for name in zipf.namelist()}\n",
    "\n",
    "def load_array(zipfile, fn):\n",
    "    return np.load(io.BytesIO(zipfile[fn]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb77a4be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb77a4be",
    "outputId": "709f5ba3-89e2-4d00-dec3-a61c777c4e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the training data:\n",
      "\n",
      "positions: (10000, 4, 2, 5)\n",
      "velocities: (10000, 1, 2, 5)\n",
      "charges: (10000, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell loads the training, validation or test data as numpy arrays,\n",
    "with the positions, initial velocities and charge data of the particles.\n",
    "\n",
    "The position arrays are shaped as\n",
    "[simulation id, time point (corresponding to t = 0, 0.5, 1 or 1.5), x/y spatial dimension, particle id].\n",
    "\n",
    "The initial velocity arrays are shaped as\n",
    "[simulation id, 1 (corresponding to t=0), x/y spatial dimension, particle id].\n",
    "\n",
    "The charge arrays are shaped as [simulation id, particle id, 1]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data = load_zip('https://surfdrive.surf.nl/files/index.php/s/OIgda2ZRG8v0eqB/download')\n",
    "\n",
    "features = ['positions', 'velocities', 'charges']\n",
    "    \n",
    "positions_train, velocities_train, charges_train = (load_array(data, f'data/train/{f}.npy') for f in features)\n",
    "positions_valid, velocities_valid, charges_valid = (load_array(data, f'data/valid/{f}.npy') for f in features)\n",
    "positions_test, velocities_test, charges_test = (load_array(data, f'data/test/{f}.npy') for f in features)\n",
    "\n",
    "print('Shapes of the training data:\\n')\n",
    "print(f'positions: {positions_train.shape}')\n",
    "print(f'velocities: {velocities_train.shape}')\n",
    "print(f'charges: {charges_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3ea4cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c3ea4cb",
    "outputId": "276125ce-2246-4ff4-e8fa-0befa892ceca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of retrieving data from the arrays:\n",
      "\n",
      "\n",
      "In simulation 42 of the training set, particle 3 with charge -1.0 had coordinates [ 2.05159559 -1.46130851].\n",
      "The initial velocity of this particle was [ 0.28402364 -0.24784824].\n"
     ]
    }
   ],
   "source": [
    "print('An example of retrieving data from the arrays:\\n\\n')\n",
    "\n",
    "sim_idx = 42\n",
    "t_idx = 2  # t_idx 0, 1, 2, 3 corresponds to t=0, 0.5, 1 and 1.5 respectively\n",
    "spatial_idx = (0,1)  # corresponds to both x and y dimension\n",
    "particle_idx = 3  # corresponds to particle with index 3\n",
    "\n",
    "p = positions_train[sim_idx, t_idx, spatial_idx, particle_idx]\n",
    "v = velocities_train[sim_idx, 0, spatial_idx, particle_idx]  # note: this array contains only the inital velocity -> hence the 0\n",
    "c = charges_train[sim_idx, particle_idx, 0] \n",
    "\n",
    "print(\n",
    "    f'In simulation {sim_idx} of the training set, particle {particle_idx} with charge {c} had coordinates {p}.\\nThe initial velocity of this particle was {v}.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a3438a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10a3438a",
    "outputId": "9bb70b84-c8ef-4a49-f4b0-6a8af182084d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of no. datapoints:\n",
      "\n",
      "10000 train, 2000 validation, 2000 test simulations\n"
     ]
    }
   ],
   "source": [
    "print('Overview of no. datapoints:\\n')\n",
    "\n",
    "print(f'{len(positions_train)} train, {len(positions_valid)} validation, {len(positions_test)} test simulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9106543",
   "metadata": {
    "id": "f9106543"
   },
   "outputs": [],
   "source": [
    "def plot_example(pos, vel):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-5., 5.])\n",
    "    axes.set_ylim([-5., 5.])\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'brown']\n",
    "    for i in range(pos.shape[-1]):\n",
    "        plt.plot(pos[0, 0, i], pos[0, 1, i], 'd', color=colors[i])\n",
    "        plt.plot(pos[-1, 0, i], pos[-1, 1, i], 'x', color=colors[i])\n",
    "        plt.plot([pos[0, 0, i], pos[0, 0, i] + vel[0, 0, i]], [pos[0, 1, i], pos[0, 1, i] + vel[0, 1, i]], '--', color=colors[i])\n",
    "    fig.set_size_inches(7, 7)\n",
    "    plt.xlim(np.min(pos)-1, np.max(pos) +1)\n",
    "    plt.ylim(np.min(pos)-1, np.max(pos) +1)\n",
    "    plt.plot([], [], 'd', color='black', label='initial position')\n",
    "    plt.plot([], [], 'x', color='black', label='final position')\n",
    "    plt.plot([], [], '--', color='black', label='initial velocity \\ndirection and magnitude')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d28681a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "d28681a6",
    "outputId": "b071f957-4e72-4f47-bc8f-797738fad547"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_idx = np.random.randint(0, 10000)\n",
    "plot_example(positions_train[random_idx], velocities_train[random_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b633c",
   "metadata": {
    "id": "059b633c"
   },
   "source": [
    "# Data Handling and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6ecb529",
   "metadata": {
    "id": "e6ecb529"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train = torch.cat((torch.tensor(positions_train[:,0,:,:]), torch.tensor(charges_train).squeeze(-1).unsqueeze(1)), dim=1)\n",
    "X_train = torch.cat((X_train, torch.tensor(velocities_train).squeeze(1)), dim=1) # shape: (simulation id, parameters (x, y, c, v_x, v_y), particle id)\n",
    "y_train = torch.tensor(positions_train[:,1:,:,:]) # shape: (simulation id, time (0.5, 1, 1.5), (x, y), particle id)\n",
    "\n",
    "X_valid = torch.cat((torch.tensor(positions_valid[:,0,:,:]), torch.tensor(charges_valid).squeeze(-1).unsqueeze(1)), dim=1)\n",
    "X_valid = torch.cat((X_valid, torch.tensor(velocities_valid).squeeze(1)), dim=1)\n",
    "y_valid = torch.tensor(positions_valid[:,1:,:,:])\n",
    "\n",
    "X_test = torch.cat((torch.tensor(positions_test[:,0,:,:]), torch.tensor(charges_test).squeeze(-1).unsqueeze(1)), dim=1)\n",
    "X_test = torch.cat((X_test, torch.tensor(velocities_test).squeeze(1)), dim=1)\n",
    "y_test = torch.tensor(positions_test[:,1:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8633eb8",
   "metadata": {
    "id": "f8633eb8"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a99a32b",
   "metadata": {
    "id": "0a99a32b"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blkK-lVfx0SK",
   "metadata": {
    "id": "blkK-lVfx0SK"
   },
   "source": [
    "# Model 1\n",
    "\n",
    "The GraphNN-like model that makes predictions based on the embedding of the set. For each timestamp we train a separate model in this section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2874d",
   "metadata": {
    "id": "18b2874d"
   },
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66774050",
   "metadata": {
    "id": "66774050"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParticleModel(torch.nn.Module):\n",
    "    def __init__(self, set_size: int = 5, input_size: int = 5, \n",
    "                 fau1_out: int = 16, gamma1_out: int = 32, fau2_out: int = 32, gamma2_out: int = 32, output_size: int = 2, device: torch.device = torch.device('cpu')):\n",
    "        super(ParticleModel, self).__init__()\n",
    "        \n",
    "        #### Getting Graph Embedding\n",
    "        \n",
    "        # first iteration \n",
    "        # layer 1\n",
    "        # input target (x_t, y_t, c_t, v_t) + neighbour (x_n, y_n, c_n, v_n) + edge (distance(t, n)) \n",
    "        # as the result input for first layer is x + x + 1 = 2x + 1 (8 + 1 in our case)\n",
    "        self.fau_iteration1 = nn.Sequential(nn.Linear(input_size*2 + 1, fau1_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 2\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_1 \n",
    "        self.gamma_iteration1 = nn.Sequential(nn.Linear(fau1_out+input_size, gamma1_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.embedding_size = gamma1_out\n",
    "\n",
    "        # second iteration \n",
    "        # layer 3\n",
    "        # input target (embedding from layer 1) + neigbour (embedding from layer 1) + edge (distance(t, n))\n",
    "        self.fau_iteration2 = nn.Sequential(nn.Linear(gamma1_out*2 + 1, fau2_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 4\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_3 \n",
    "        self.gamma_iteration2 = nn.Sequential(nn.Linear(fau2_out + gamma1_out, gamma2_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.output_size = gamma2_out\n",
    "        self.device = device\n",
    "        self.prediction_size = output_size\n",
    "        self.final_layer = nn.Sequential(nn.Linear(gamma2_out*set_size, output_size*set_size))\n",
    "\n",
    "    \n",
    "    def forward_iteration1(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[1], self.embedding_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration1(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate \n",
    "            # x of shape (batch_size, 1, num_features+fau_output)\n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            \n",
    "            embeddings[:,i] = self.gamma_iteration1(x).view(particle_set.shape[0], -1)\n",
    "        # for convenicence, swap axes when return\n",
    "        return embeddings.swapaxes(1,2)\n",
    "\n",
    "    def forward_iteration2(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[2], self.output_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration2(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate the neighborhood and \n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            embeddings[:,i] = self.gamma_iteration2(x).view(particle_set.shape[0], -1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, particle_set):\n",
    "        distances = torch.stack([torch.cdist(x_i, x_i) for x_i in particle_set.swapaxes(1,2)], dim=0).to(self.device)\n",
    "        inner_emedding = self.forward_iteration1(particle_set = particle_set, distances = distances).to(self.device)\n",
    "        embedding = self.forward_iteration2(particle_set = inner_emedding, distances = distances)\n",
    "        \n",
    "        prediction = self.final_layer(embedding.view(embedding.shape[0], -1))\n",
    "\n",
    "        return prediction.view(embedding.shape[0], self.prediction_size, -1) # (particle_set.shape[0], 5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea70d73",
   "metadata": {
    "id": "dea70d73"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e95af5f9",
   "metadata": {
    "id": "e95af5f9"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class ParticleDistanceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "      The loss to calculate mean distance between predicted location of particle of each set.\n",
    "      By defaul, Euclidean distance is set\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, norm: float = 2):\n",
    "        super(ParticleDistanceLoss, self).__init__()\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, input_set, target_set):\n",
    "        \"\"\"\n",
    "            (batch_size, coordinates, set_size)\n",
    "        \"\"\"\n",
    "        return (input_set - target_set).norm(p = self.norm, dim = 1).mean(axis = 1).mean()\n",
    "        \n",
    "        \n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_DataLoader: torch.utils.data.Dataset,\n",
    "                 validation_DataLoader: torch.utils.data.Dataset ,\n",
    "                 testing_DataLoader: torch.utils.data.Dataset ,\n",
    "                 epochs: int\n",
    "                 ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.training_DataLoader = training_DataLoader\n",
    "        self.validation_DataLoader = validation_DataLoader\n",
    "        self.testing_DataLoader = testing_DataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "    def run_trainer(self, target_time):\n",
    "\n",
    "        epoch_train_losses = []\n",
    "        epoch_val_losses = []\n",
    "\n",
    "        times = [0.5, 1, 1.5]\n",
    "        time_to_ind = {0.5:0, 1:1, 1.5:2}\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "                 \n",
    "\n",
    "            self.model.train()  # train mode\n",
    "\n",
    "            train_losses=[]\n",
    "            \n",
    "            correct = 0\n",
    "            length = 0\n",
    "            for x, y in self.training_DataLoader:\n",
    "\n",
    "                A, B = x.float().to(self.device), y.float().to(self.device) # send to device (GPU or CPU)\n",
    "\n",
    "                self.optimizer.zero_grad()  # zerograd the parameters\n",
    "\n",
    "                loss = 0\n",
    "                out = self.model(A)  # one forward pass\n",
    "                loss += self.criterion(out, B[:,time_to_ind[target_time]])  # calculate loss\n",
    "                \n",
    "                loss_value = loss.item()\n",
    "                train_losses.append(loss_value)\n",
    "                 \n",
    "                loss.backward()  # one backward pass\n",
    "                self.optimizer.step()  # update the parameters\n",
    "            \n",
    "            epoch_train_losses.append(np.mean(train_losses))\n",
    "            self.model.eval()  # evaluation mode\n",
    "            valid_losses = []  # accumulate the losses here\n",
    "\n",
    "            correct = 0\n",
    "            length = 0\n",
    "            for x,  y in self.validation_DataLoader:\n",
    "\n",
    "                A,B = x.float().to(self.device), y.float().to(self.device) # send to device (GPU or CPU)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    loss = 0\n",
    "                    \n",
    "                    out = self.model(A)  # one forward pass\n",
    "                    loss += self.criterion(out, B[:,time_to_ind[target_time]])  # calculate loss\n",
    "                 \n",
    "                    loss_value = loss.item()\n",
    "                    valid_losses.append(loss_value)\n",
    "\n",
    "            epoch_val_losses.append(np.mean(valid_losses))\n",
    "                \n",
    "            # print the results\n",
    "            print(\n",
    "                f'EPOCH: {epoch+1:0>{len(str(self.epochs))}}/{self.epochs}',\n",
    "                end=' '\n",
    "            )\n",
    "            print(f'LOSS: {np.mean(train_losses):.4f}',end=' ')\n",
    "            print(f'VAL-LOSS: {np.mean(valid_losses):.4f}',end='\\n')\n",
    "\n",
    "        return epoch_train_losses, epoch_val_losses\n",
    "        \n",
    "    def evaluate(self, target_time):\n",
    "\n",
    "        self.model.eval()\n",
    "        times = [0.5, 1, 1.5]\n",
    "        time_to_ind = {0.5:0, 1:1, 1.5:2}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss = []\n",
    "            length = 0\n",
    "            for x,y in self.testing_DataLoader:\n",
    "                \n",
    "                A, B = x.float().to(self.device), y.float().to(self.device)\n",
    "                out = self.model(A)\n",
    "                loss.append(self.criterion(out, B[:,time_to_ind[target_time]]))\n",
    "        print(f'Error: {np.mean(loss):.4f} for target time {target_time}',end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bPfkmpEqnT",
   "metadata": {
    "id": "d9bPfkmpEqnT"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O3jtlEzZEwTE",
   "metadata": {
    "id": "O3jtlEzZEwTE"
   },
   "source": [
    "### Training with max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07e03ddf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07e03ddf",
    "outputId": "53ff3afb-6f7a-435a-ccbf-306999bf00e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:03<01:00,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 0.5790 VAL-LOSS: 0.2541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 2/20 [00:06<00:56,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.2283 VAL-LOSS: 0.2145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▍                                                                      | 3/20 [00:09<00:53,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.2026 VAL-LOSS: 0.2184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 4/20 [00:12<00:51,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.1930 VAL-LOSS: 0.1828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████▊                                                              | 5/20 [00:15<00:48,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.1807 VAL-LOSS: 0.1752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 6/20 [00:19<00:44,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.1728 VAL-LOSS: 0.1631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|█████████████████████████████                                                      | 7/20 [00:22<00:40,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.1722 VAL-LOSS: 0.1687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 8/20 [00:25<00:37,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.1706 VAL-LOSS: 0.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████████▎                                             | 9/20 [00:28<00:34,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.1684 VAL-LOSS: 0.1596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████                                         | 10/20 [00:31<00:31,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.1616 VAL-LOSS: 0.1698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████████████████████████                                     | 11/20 [00:34<00:28,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.1671 VAL-LOSS: 0.1695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▏                                | 12/20 [00:38<00:25,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.1626 VAL-LOSS: 0.1648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|█████████████████████████████████████████████████████▎                            | 13/20 [00:41<00:23,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.1642 VAL-LOSS: 0.1642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████▍                        | 14/20 [00:45<00:19,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.1602 VAL-LOSS: 0.1690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 15/20 [00:48<00:16,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.1611 VAL-LOSS: 0.1512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 16/20 [00:51<00:13,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.1648 VAL-LOSS: 0.1591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 17/20 [00:55<00:09,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.1580 VAL-LOSS: 0.1654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 18/20 [00:58<00:06,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.1572 VAL-LOSS: 0.1627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████████▉    | 19/20 [01:01<00:03,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.1587 VAL-LOSS: 0.1673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [01:04<00:00,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.1568 VAL-LOSS: 0.1603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.1620 for target time 0.5 "
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "model = ParticleModel(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer(target_time=0.5)\n",
    "TrainingProcedure.evaluate(target_time=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0f24a09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0f24a09",
    "outputId": "772c9a7d-7100-4693-a46b-cdcec564f862"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:03<01:01,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 0.7870 VAL-LOSS: 0.4562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 2/20 [00:06<00:58,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.4079 VAL-LOSS: 0.3825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▍                                                                      | 3/20 [00:09<00:56,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.3533 VAL-LOSS: 0.3435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 4/20 [00:13<00:52,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.3313 VAL-LOSS: 0.3084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████▊                                                              | 5/20 [00:16<00:48,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.3232 VAL-LOSS: 0.3109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 6/20 [00:19<00:43,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.3067 VAL-LOSS: 0.3196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|█████████████████████████████                                                      | 7/20 [00:21<00:38,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.3041 VAL-LOSS: 0.2825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 8/20 [00:24<00:35,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.2965 VAL-LOSS: 0.2897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████████▎                                             | 9/20 [00:27<00:30,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.2923 VAL-LOSS: 0.2888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████                                         | 10/20 [00:29<00:25,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.2931 VAL-LOSS: 0.2907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████████████████████████                                     | 11/20 [00:31<00:21,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.2796 VAL-LOSS: 0.2776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▏                                | 12/20 [00:33<00:18,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.2811 VAL-LOSS: 0.2774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|█████████████████████████████████████████████████████▎                            | 13/20 [00:35<00:16,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.2800 VAL-LOSS: 0.2809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████▍                        | 14/20 [00:37<00:13,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.2753 VAL-LOSS: 0.2687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 15/20 [00:40<00:11,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.2710 VAL-LOSS: 0.2866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 16/20 [00:42<00:08,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.2720 VAL-LOSS: 0.2679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 17/20 [00:44<00:06,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.2674 VAL-LOSS: 0.2612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 18/20 [00:46<00:04,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.2638 VAL-LOSS: 0.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████████▉    | 19/20 [00:48<00:02,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.2599 VAL-LOSS: 0.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:50<00:00,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.2572 VAL-LOSS: 0.2709\n",
      "Error: 0.2788 for target time 1 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ParticleModel(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer(target_time=1)\n",
    "TrainingProcedure.evaluate(target_time=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f4f81969",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4f81969",
    "outputId": "590b4283-886f-477e-8818-9b838c1fbea2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:02<00:40,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 0.9779 VAL-LOSS: 0.5888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:04<00:38,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.5174 VAL-LOSS: 0.5361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:06<00:39,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.4793 VAL-LOSS: 0.4753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:10<00:43,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.4591 VAL-LOSS: 0.4479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:13<00:45,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.4446 VAL-LOSS: 0.4522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:19<00:53,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.4367 VAL-LOSS: 0.4370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:22<00:46,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.4205 VAL-LOSS: 0.4334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:24<00:37,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.4131 VAL-LOSS: 0.4124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:26<00:31,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.4050 VAL-LOSS: 0.4181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:28<00:25,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.3932 VAL-LOSS: 0.3855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:30<00:22,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.3806 VAL-LOSS: 0.3836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:32<00:18,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.3739 VAL-LOSS: 0.3816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:34<00:15,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.3679 VAL-LOSS: 0.3610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:37<00:13,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.3584 VAL-LOSS: 0.3465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:39<00:11,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.3534 VAL-LOSS: 0.3607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:41<00:08,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.3515 VAL-LOSS: 0.3632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:43<00:06,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.3495 VAL-LOSS: 0.3696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:46<00:04,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.3469 VAL-LOSS: 0.3480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:49<00:02,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.3477 VAL-LOSS: 0.3518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:51<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.3395 VAL-LOSS: 0.3385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.3471 "
     ]
    }
   ],
   "source": [
    "model = ParticleModel(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer(target_time=1.5)\n",
    "TrainingProcedure.evaluate(target_time=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf8f64",
   "metadata": {
    "id": "b5cf8f64"
   },
   "source": [
    "### Linear interpolation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c619cd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c619cd3",
    "outputId": "c3d56479-27a9-4e0f-c7f2-8440cffeec66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of mean distance (Eudclidean) for 0.5 seconds: 0.1682; for 1.0 second: 0.3932, for 1.5 seconds: 0.6314 "
     ]
    }
   ],
   "source": [
    "# (simulation id, time (0.5, 1, 1.5), (x, y), particle id)\n",
    "# (simulation id, parameters (x, y, c, v_x, v_y), particle id)\n",
    "\n",
    "def predict(x, time):\n",
    "    predictions = torch.cat([x[:,0,:]+ time*x[:,-2,:], x[:,1,:] + time*x[:,-1,:]], dim = 1)\n",
    "    return predictions.view((x.shape[0], -1, x.shape[-1]))\n",
    "\n",
    "loss_f = ParticleDistanceLoss(norm = 2)\n",
    "\n",
    "times = [0.5, 1, 1.5]\n",
    "time_to_ind = {0.5:0, 1:1, 1.5:2}\n",
    "loss = [[],[],[]]\n",
    "for x,y in test_dataloader:\n",
    "    for time in times:                    \n",
    "        A, B = x.float(), y.float()\n",
    "        out = predict(A, time)\n",
    "        loss[time_to_ind[time]].append(loss_f(out, B[:,time_to_ind[time]]))\n",
    "\n",
    "print(f'Mean of mean distance (Eudclidean) for 0.5 seconds: {np.mean(loss[0]):.4f}; for 1.0 second: {np.mean(loss[1]):.4f}, for 1.5 seconds: {np.mean(loss[2]):.4f}',end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qt1bd2y9C760",
   "metadata": {
    "id": "qt1bd2y9C760"
   },
   "source": [
    "### Training with Mean Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "Kc9LFwZICSx1",
   "metadata": {
    "id": "Kc9LFwZICSx1"
   },
   "outputs": [],
   "source": [
    "class ParticleModel(torch.nn.Module):\n",
    "    def __init__(self, set_size: int = 5, input_size: int = 5, \n",
    "                 fau1_out: int = 16, gamma1_out: int = 32, fau2_out: int = 32, gamma2_out: int = 32, output_size: int = 2, device: torch.device = torch.device('cpu')):\n",
    "        super(ParticleModel, self).__init__()\n",
    "        \n",
    "        #### Getting Graph Embedding\n",
    "        \n",
    "        # first iteration \n",
    "        # layer 1\n",
    "        # input target (x_t, y_t, c_t, v_t) + neighbour (x_n, y_n, c_n, v_n) + edge (distance(t, n)) \n",
    "        # as the result input for first layer is x + x + 1 = 2x + 1 (8 + 1 in our case)\n",
    "        self.fau_iteration1 = nn.Sequential(nn.Linear(input_size*2 + 1, fau1_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 2\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_1 \n",
    "        self.gamma_iteration1 = nn.Sequential(nn.Linear(fau1_out+input_size, gamma1_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.embedding_size = gamma1_out\n",
    "\n",
    "        # second iteration \n",
    "        # layer 3\n",
    "        # input target (embedding from layer 1) + neigbour (embedding from layer 1) + edge (distance(t, n))\n",
    "        self.fau_iteration2 = nn.Sequential(nn.Linear(gamma1_out*2 + 1, fau2_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 4\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_3 \n",
    "        self.gamma_iteration2 = nn.Sequential(nn.Linear(fau2_out + gamma1_out, gamma2_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.output_size = gamma2_out\n",
    "        self.device = device\n",
    "        self.prediction_size = output_size\n",
    "        self.final_layer = nn.Sequential(nn.Linear(gamma2_out*set_size, output_size*set_size))\n",
    "\n",
    "    \n",
    "    def forward_iteration1(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[1], self.embedding_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration1(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.mean(axis = 1)\n",
    "\n",
    "            # concatenate \n",
    "            # x of shape (batch_size, 1, num_features+fau_output)\n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            \n",
    "            embeddings[:,i] = self.gamma_iteration1(x).view(particle_set.shape[0], -1)\n",
    "        # for convenicence, swap axes when return\n",
    "        return embeddings.swapaxes(1,2)\n",
    "\n",
    "    def forward_iteration2(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[2], self.output_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration2(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.mean(axis = 1)\n",
    "\n",
    "            # concatenate the neighborhood and \n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            embeddings[:,i] = self.gamma_iteration2(x).view(particle_set.shape[0], -1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, particle_set):\n",
    "        distances = torch.stack([torch.cdist(x_i, x_i) for x_i in particle_set.swapaxes(1,2)], dim=0).to(self.device)\n",
    "        inner_emedding = self.forward_iteration1(particle_set = particle_set, distances = distances).to(self.device)\n",
    "        embedding = self.forward_iteration2(particle_set = inner_emedding, distances = distances)\n",
    "        \n",
    "        prediction = self.final_layer(embedding.view(embedding.shape[0], -1))\n",
    "\n",
    "        return prediction.view(embedding.shape[0], self.prediction_size, -1) # (particle_set.shape[0], 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ZwZsl0pDChJt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwZsl0pDChJt",
    "outputId": "594870eb-edbc-43f2-fafb-2cca22bf59f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:02<00:38,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 0.5408 VAL-LOSS: 0.2548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:04<00:36,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.2278 VAL-LOSS: 0.2168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:05<00:33,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.2030 VAL-LOSS: 0.1901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:07<00:31,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.1887 VAL-LOSS: 0.1950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:09<00:29,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.1837 VAL-LOSS: 0.1716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:11<00:27,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.1760 VAL-LOSS: 0.1985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:13<00:25,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.1757 VAL-LOSS: 0.1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:15<00:23,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.1736 VAL-LOSS: 0.1863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:17<00:21,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.1705 VAL-LOSS: 0.1723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:19<00:19,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.1696 VAL-LOSS: 0.1871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:21<00:17,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.1709 VAL-LOSS: 0.1595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:23<00:16,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.1663 VAL-LOSS: 0.1837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:25<00:13,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.1679 VAL-LOSS: 0.1836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:27<00:11,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.1649 VAL-LOSS: 0.1563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:29<00:09,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.1648 VAL-LOSS: 0.1630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:31<00:07,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.1648 VAL-LOSS: 0.1704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:33<00:05,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.1651 VAL-LOSS: 0.1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:35<00:03,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.1623 VAL-LOSS: 0.1781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:37<00:01,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.1608 VAL-LOSS: 0.1519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:39<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.1569 VAL-LOSS: 0.1558\n",
      "Error: 0.1591 for target time 0.5 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:02<00:38,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 0.7293 VAL-LOSS: 0.4300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:04<00:36,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.4023 VAL-LOSS: 0.3737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:06<00:34,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.3513 VAL-LOSS: 0.3524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:08<00:32,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.3376 VAL-LOSS: 0.3355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:10<00:29,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.3318 VAL-LOSS: 0.3349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:12<00:27,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.3248 VAL-LOSS: 0.3191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:14<00:26,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.3146 VAL-LOSS: 0.3214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:16<00:24,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.3145 VAL-LOSS: 0.3186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:18<00:22,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.3098 VAL-LOSS: 0.3069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:20<00:19,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.3082 VAL-LOSS: 0.3103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:22<00:17,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.3022 VAL-LOSS: 0.3212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:24<00:15,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.3045 VAL-LOSS: 0.2877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:26<00:14,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.2994 VAL-LOSS: 0.3045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:28<00:12,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.2956 VAL-LOSS: 0.2915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:30<00:10,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.2892 VAL-LOSS: 0.2936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:32<00:08,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.2885 VAL-LOSS: 0.2819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:34<00:06,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.2859 VAL-LOSS: 0.2871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:36<00:04,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.2799 VAL-LOSS: 0.2794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:38<00:02,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.2772 VAL-LOSS: 0.2846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:40<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.2737 VAL-LOSS: 0.2693\n",
      "Error: 0.2766 for target time 1 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:01<00:37,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 0.9329 VAL-LOSS: 0.5707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:03<00:35,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.5407 VAL-LOSS: 0.5211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:05<00:33,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.5021 VAL-LOSS: 0.5183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:07<00:31,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.4818 VAL-LOSS: 0.4719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:09<00:29,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.4746 VAL-LOSS: 0.4573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:11<00:27,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.4638 VAL-LOSS: 0.4763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:13<00:25,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.4514 VAL-LOSS: 0.4665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:15<00:23,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.4447 VAL-LOSS: 0.4427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:17<00:21,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.4385 VAL-LOSS: 0.4313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:19<00:19,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.4277 VAL-LOSS: 0.4697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:21<00:17,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.4284 VAL-LOSS: 0.4302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:23<00:15,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.4173 VAL-LOSS: 0.4454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:25<00:13,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.4152 VAL-LOSS: 0.4289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:27<00:11,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.4057 VAL-LOSS: 0.4157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:29<00:09,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.4002 VAL-LOSS: 0.4038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:31<00:07,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.3958 VAL-LOSS: 0.3947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:33<00:05,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.3896 VAL-LOSS: 0.3925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:35<00:03,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.3820 VAL-LOSS: 0.4133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:37<00:01,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.3781 VAL-LOSS: 0.3833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:39<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.3773 VAL-LOSS: 0.3861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.3930 for target time 1.5 "
     ]
    }
   ],
   "source": [
    "times = [0.5, 1, 1.5]\n",
    "time_to_ind = {0.5:0, 1:1, 1.5:2}\n",
    "\n",
    "for time in times:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device=torch.device('cpu')\n",
    "\n",
    "    model = ParticleModel(device = device).to(device)\n",
    "    criterion = ParticleDistanceLoss(norm=2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    TrainingProcedure = Trainer(model, \n",
    "                                device, \n",
    "                                criterion, \n",
    "                                optimizer,\n",
    "                                train_dataloader,\n",
    "                                valid_dataloader,\n",
    "                                test_dataloader,\n",
    "                                epochs = 20)\n",
    "\n",
    "    train_loss, val_loss = TrainingProcedure.run_trainer(target_time=time)\n",
    "    TrainingProcedure.evaluate(target_time=time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GEZhrF4wEDI0",
   "metadata": {
    "id": "GEZhrF4wEDI0"
   },
   "source": [
    "### Results comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZVQNHIGCD0ME",
   "metadata": {
    "id": "ZVQNHIGCD0ME"
   },
   "source": [
    "#### Comparison\n",
    "\n",
    "**Mean pooling**:\n",
    "\n",
    "Mean distance (Eudclidean) for 0.5 seconds: 0.1591; for 1.0 second: 0.2766, for 1.5 seconds: 0.3930\n",
    "\n",
    "**Max pooling**: \n",
    "\n",
    "Mean distance (Eudclidean) for 0.5 seconds: 0.1558; for 1.0 second: 0.2641, for 1.5 seconds: 0.3471\n",
    "\n",
    "**Linear interpolation**:\n",
    "\n",
    "Mean distance (Eudclidean) for 0.5 seconds: 0.1682; for 1.0 second: 0.3932, for 1.5 seconds: 0.6314 \n",
    "\n",
    "As the result, simple model provides more precise prediction as the time goe than a linear interpolation by the formula $ x^t_i = x^0_i + v^0_i*t $, where $x$ is the coordinate and $v$ is velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12U9ShwSGi9a",
   "metadata": {
    "id": "12U9ShwSGi9a"
   },
   "source": [
    "# Model 2\n",
    "\n",
    "The model with GraphNN-like embedding extraction followed by the LSTM. That model is considered to make prediction based on previous prediction and thus creates sequence of predictions for next **n** timestamps with step-size of 0.5 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w3kzTcgQHHdg",
   "metadata": {
    "id": "w3kzTcgQHHdg"
   },
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8UP7RPOBG7R8",
   "metadata": {
    "id": "8UP7RPOBG7R8"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParticleModelLSTM(torch.nn.Module):\n",
    "    def __init__(self, set_size: int = 5, input_size: int = 5, \n",
    "                 fau1_out: int = 16, gamma1_out: int = 32, fau2_out: int = 32, gamma2_out: int = 32, output_size: int = 2,\n",
    "                 hidden_dim: int = 32, horizon_length: int = 3,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        super(ParticleModelLSTM, self).__init__()\n",
    "        \n",
    "        #### Getting Graph Embedding\n",
    "        \n",
    "        # first iteration \n",
    "        # layer 1\n",
    "        # input target (x_t, y_t, c_t, v_t) + neighbour (x_n, y_n, c_n, v_n) + edge (distance(t, n)) \n",
    "        # as the result input for first layer is x + x + 1 = 2x + 1 (8 + 1 in our case)\n",
    "        self.fau_iteration1 = nn.Sequential(nn.Linear(input_size*2 + 1, fau1_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 2\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_1 \n",
    "        self.gamma_iteration1 = nn.Sequential(nn.Linear(fau1_out+input_size, gamma1_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.embedding_size = gamma1_out\n",
    "\n",
    "        # second iteration \n",
    "        # layer 3\n",
    "        # input target (embedding from layer 1) + neigbour (embedding from layer 1) + edge (distance(t, n))\n",
    "        self.fau_iteration2 = nn.Sequential(nn.Linear(gamma1_out*2 + 1, fau2_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 4\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_3 \n",
    "        self.gamma_iteration2 = nn.Sequential(nn.Linear(fau2_out + gamma1_out, gamma2_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        \n",
    "        #self.final_layer = nn.Sequential(nn.Linear(gamma2_out*set_size, output_size*set_size))\n",
    "\n",
    "         # Recurrent Part\n",
    "        self.rnn = nn.LSTM(gamma2_out*set_size, hidden_dim, num_layers = 1, batch_first = True)\n",
    "        \n",
    "        # Fully connected layer after RNN output, to learn more complex relations\n",
    "        self.final = nn.Linear(hidden_dim, output_size*set_size)\n",
    "\n",
    "\n",
    "        # auxiliary variables \n",
    "        self.output_size = gamma2_out\n",
    "        self.device = device\n",
    "        self.prediction_size = output_size\n",
    "        self.horizon_length = horizon_length\n",
    "        self.set_size = set_size\n",
    "\n",
    "    \n",
    "    def forward_iteration1(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[1], self.embedding_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration1(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate \n",
    "            # x of shape (batch_size, 1, num_features+fau_output)\n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            \n",
    "            embeddings[:,i] = self.gamma_iteration1(x).view(particle_set.shape[0], -1)\n",
    "        # for convenicence, swap axes when return\n",
    "        return embeddings.swapaxes(1,2)\n",
    "\n",
    "    def forward_iteration2(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[2], self.output_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration2(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate the neighborhood and \n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            embeddings[:,i] = self.gamma_iteration2(x).view(particle_set.shape[0], -1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, particle_set):\n",
    "        distances = torch.stack([torch.cdist(x_i, x_i) for x_i in particle_set.swapaxes(1,2)], dim=0).to(self.device)\n",
    "        inner_emedding = self.forward_iteration1(particle_set = particle_set, distances = distances).to(self.device)\n",
    "        embedding = self.forward_iteration2(particle_set = inner_emedding, distances = distances)\n",
    "        zeros = torch.zeros((embedding.shape[0], self.horizon_length, embedding.shape[-1]*self.set_size)).to(self.device) \n",
    "        #(batch_size, 1, embedding_dim)\n",
    "        rnn_input = torch.cat([embedding.view((embedding.shape[0], 1, -1)), zeros], dim=1)\n",
    "        # Input: (Batch size, L (sequence length), input_shape)\n",
    "        # rnn_output: (Batch size, L (sequence length), output_shape)\n",
    "        rnn_output, rnn_state = self.rnn(rnn_input)\n",
    "\n",
    "        output = self.final(rnn_output[:,1:,:])\n",
    "\n",
    "        return output.view((output.shape[0], self.horizon_length, self.prediction_size, self.set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o09IWs0mKmaA",
   "metadata": {
    "id": "o09IWs0mKmaA"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fx-o9sRWIl0d",
   "metadata": {
    "id": "fx-o9sRWIl0d"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_DataLoader: torch.utils.data.Dataset,\n",
    "                 validation_DataLoader: torch.utils.data.Dataset ,\n",
    "                 testing_DataLoader: torch.utils.data.Dataset ,\n",
    "                 epochs: int\n",
    "                 ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.training_DataLoader = training_DataLoader\n",
    "        self.validation_DataLoader = validation_DataLoader\n",
    "        self.testing_DataLoader = testing_DataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "    def run_trainer(self):\n",
    "\n",
    "        epoch_train_losses = []\n",
    "        epoch_val_losses = []\n",
    "\n",
    "        times = [0.5, 1, 1.5]\n",
    "        time_to_ind = {0.5:0, 1:1, 1.5:2}\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "                 \n",
    "\n",
    "            self.model.train()  # train mode\n",
    "\n",
    "            train_losses=[]\n",
    "            \n",
    "            for x, y in self.training_DataLoader:\n",
    "\n",
    "                x, y = x.float().to(self.device), y.float().to(self.device) # send to device (GPU or CPU)\n",
    "\n",
    "                self.optimizer.zero_grad()  # zerograd the parameters\n",
    "\n",
    "                out = self.model(x)\n",
    "                loss = 0\n",
    "                for i in range(3):\n",
    "                    loss += self.criterion(y[:,i], out[:,i])\n",
    "                \n",
    "                loss_value = loss.item()/3\n",
    "                train_losses.append(loss_value)\n",
    "                 \n",
    "                loss.backward()  # one backward pass\n",
    "                self.optimizer.step()  # update the parameters\n",
    "            \n",
    "            epoch_train_losses.append(np.mean(train_losses))\n",
    "            self.model.eval()  # evaluation mode\n",
    "            valid_losses = []  # accumulate the losses here\n",
    "\n",
    "            correct = 0\n",
    "            length = 0\n",
    "            for x,  y in self.validation_DataLoader:\n",
    "\n",
    "                x,y = x.float().to(self.device), y.float().to(self.device) # send to device (GPU or CPU)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    out = self.model(x)\n",
    "                    loss = 0\n",
    "                    for i in range(3):\n",
    "                        loss += self.criterion(y[:,i], out[:,i])\n",
    "                    \n",
    "                    loss_value = loss.item()/3\n",
    "                    valid_losses.append(loss_value)\n",
    "\n",
    "            epoch_val_losses.append(np.mean(valid_losses))\n",
    "                \n",
    "            # print the results\n",
    "            print(\n",
    "                f'EPOCH: {epoch+1:0>{len(str(self.epochs))}}/{self.epochs}',\n",
    "                end=' '\n",
    "            )\n",
    "            print(f'LOSS: {np.mean(train_losses):.4f}',end=' ')\n",
    "            print(f'VAL-LOSS: {np.mean(valid_losses):.4f}',end='\\n')\n",
    "\n",
    "        return epoch_train_losses, epoch_val_losses\n",
    "        \n",
    "    def evaluate(self):\n",
    "\n",
    "        self.model.eval()\n",
    "        times = [0.5, 1, 1.5]\n",
    "        time_to_ind = {0.5:0, 1:1, 1.5:2}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_separate = [[],[],[]]\n",
    "            overall_loss = []\n",
    "            for x,y in self.testing_DataLoader:\n",
    "\n",
    "                x,y = x.float().to(self.device), y.float().to(self.device)\n",
    "                out = self.model(x)\n",
    "\n",
    "                loss = 0\n",
    "                for i in range(3):\n",
    "                    loss_separate[i].append(self.criterion(y[:,i], out[:,i]).item())\n",
    "                    loss+=loss_separate[i][-1]\n",
    "                overall_loss.append(loss/3)\n",
    "\n",
    "        print(f'Error for 0.5 seconds: {np.mean(loss_separate[0]):.4f}; for 1.0 second: {np.mean(loss_separate[1]):.4f}, for 1.5 seconds: {np.mean(loss_separate[2]):.4f}')\n",
    "        print(f'Error over all time (training like): {np.mean(overall_loss):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lCqMj7lwMvQv",
   "metadata": {
    "id": "lCqMj7lwMvQv"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I_Z-Jfm4M1gy",
   "metadata": {
    "id": "I_Z-Jfm4M1gy"
   },
   "source": [
    "### Training with max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2ZFQz0CzM2nE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ZFQz0CzM2nE",
    "outputId": "0c18ed1f-6d41-49ef-a4f5-3a7ecff5c9f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:02<00:50,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 1.7652 VAL-LOSS: 0.8620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:05<00:47,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.7043 VAL-LOSS: 0.6117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:07<00:44,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.5594 VAL-LOSS: 0.5188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:10<00:41,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.4992 VAL-LOSS: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:12<00:38,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.4644 VAL-LOSS: 0.4862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:15<00:36,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.4506 VAL-LOSS: 0.4815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:18<00:33,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.4385 VAL-LOSS: 0.4363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:20<00:31,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.4241 VAL-LOSS: 0.4126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:23<00:28,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.4140 VAL-LOSS: 0.4274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:25<00:25,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.4062 VAL-LOSS: 0.4057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:28<00:23,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.3967 VAL-LOSS: 0.3998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:31<00:20,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.3871 VAL-LOSS: 0.3998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:33<00:18,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.3808 VAL-LOSS: 0.3933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:36<00:15,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.3781 VAL-LOSS: 0.3781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:38<00:13,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.3690 VAL-LOSS: 0.3769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:41<00:10,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.3682 VAL-LOSS: 0.3776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:44<00:07,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.3699 VAL-LOSS: 0.3714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:46<00:05,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.3607 VAL-LOSS: 0.3550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:49<00:02,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.3618 VAL-LOSS: 0.3717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:52<00:00,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.3595 VAL-LOSS: 0.3682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for 0.5 seconds: 0.2818; for 1.0 second: 0.3414, for 1.5 seconds: 0.4955 Error over all time (training like): 0.3729\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "model = ParticleModelLSTM(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer()\n",
    "TrainingProcedure.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-KQsWYrUP6hm",
   "metadata": {
    "id": "-KQsWYrUP6hm"
   },
   "source": [
    "### Training with mean pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T_QraFA7P_G3",
   "metadata": {
    "id": "T_QraFA7P_G3"
   },
   "source": [
    "#### code changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4eP047vsP-d9",
   "metadata": {
    "id": "4eP047vsP-d9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParticleModelLSTM(torch.nn.Module):\n",
    "    def __init__(self, set_size: int = 5, input_size: int = 5, \n",
    "                 fau1_out: int = 16, gamma1_out: int = 32, fau2_out: int = 32, gamma2_out: int = 32, output_size: int = 2,\n",
    "                 hidden_dim: int = 32, horizon_length: int = 3,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        super(ParticleModelLSTM, self).__init__()\n",
    "        \n",
    "        #### Getting Graph Embedding\n",
    "        \n",
    "        # first iteration \n",
    "        # layer 1\n",
    "        # input target (x_t, y_t, c_t, v_t) + neighbour (x_n, y_n, c_n, v_n) + edge (distance(t, n)) \n",
    "        # as the result input for first layer is x + x + 1 = 2x + 1 (8 + 1 in our case)\n",
    "        self.fau_iteration1 = nn.Sequential(nn.Linear(input_size*2 + 1, fau1_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 2\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_1 \n",
    "        self.gamma_iteration1 = nn.Sequential(nn.Linear(fau1_out+input_size, gamma1_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.embedding_size = gamma1_out\n",
    "\n",
    "        # second iteration \n",
    "        # layer 3\n",
    "        # input target (embedding from layer 1) + neigbour (embedding from layer 1) + edge (distance(t, n))\n",
    "        self.fau_iteration2 = nn.Sequential(nn.Linear(gamma1_out*2 + 1, fau2_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 4\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_3 \n",
    "        self.gamma_iteration2 = nn.Sequential(nn.Linear(fau2_out + gamma1_out, gamma2_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        \n",
    "        #self.final_layer = nn.Sequential(nn.Linear(gamma2_out*set_size, output_size*set_size))\n",
    "\n",
    "         # Recurrent Part\n",
    "        self.rnn = nn.LSTM(gamma2_out*set_size, hidden_dim, num_layers = 1, batch_first = True)\n",
    "        \n",
    "        # Fully connected layer after RNN output, to learn more complex relations\n",
    "        self.final = nn.Linear(hidden_dim, output_size*set_size)\n",
    "\n",
    "\n",
    "        # auxiliary variables \n",
    "        self.output_size = gamma2_out\n",
    "        self.device = device\n",
    "        self.prediction_size = output_size\n",
    "        self.horizon_length = horizon_length\n",
    "        self.set_size = set_size\n",
    "\n",
    "    \n",
    "    def forward_iteration1(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[1], self.embedding_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration1(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.mean(axis = 1)\n",
    "\n",
    "            # concatenate \n",
    "            # x of shape (batch_size, 1, num_features+fau_output)\n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            \n",
    "            embeddings[:,i] = self.gamma_iteration1(x).view(particle_set.shape[0], -1)\n",
    "        # for convenicence, swap axes when return\n",
    "        return embeddings.swapaxes(1,2)\n",
    "\n",
    "    def forward_iteration2(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[2], self.output_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration2(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.mean(axis = 1)\n",
    "\n",
    "            # concatenate the neighborhood and \n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            embeddings[:,i] = self.gamma_iteration2(x).view(particle_set.shape[0], -1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, particle_set):\n",
    "        distances = torch.stack([torch.cdist(x_i, x_i) for x_i in particle_set.swapaxes(1,2)], dim=0).to(self.device)\n",
    "        inner_emedding = self.forward_iteration1(particle_set = particle_set, distances = distances).to(self.device)\n",
    "        embedding = self.forward_iteration2(particle_set = inner_emedding, distances = distances)\n",
    "        zeros = torch.zeros((embedding.shape[0], self.horizon_length, embedding.shape[-1]*self.set_size)).to(self.device) \n",
    "        #(batch_size, 1, embedding_dim)\n",
    "        rnn_input = torch.cat([embedding.view((embedding.shape[0], 1, -1)), zeros], dim=1)\n",
    "        # Input: (Batch size, L (sequence length), input_shape)\n",
    "        # rnn_output: (Batch size, L (sequence length), output_shape)\n",
    "        rnn_output, rnn_state = self.rnn(rnn_input)\n",
    "\n",
    "        output = self.final(rnn_output[:,1:,:])\n",
    "\n",
    "        return output.view((output.shape[0], self.horizon_length, self.prediction_size, self.set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s0EbjfQ0QDFs",
   "metadata": {
    "id": "s0EbjfQ0QDFs"
   },
   "source": [
    "#### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "PBzu6uW3P8rA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBzu6uW3P8rA",
    "outputId": "5850936a-d9c6-4f9f-9c42-32b75f9d450e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:03<01:01,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 1.7670 VAL-LOSS: 0.9098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:05<00:51,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.7038 VAL-LOSS: 0.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:08<00:46,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.5671 VAL-LOSS: 0.5297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:11<00:43,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.5052 VAL-LOSS: 0.4985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:13<00:39,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.4710 VAL-LOSS: 0.4729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:16<00:36,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.4531 VAL-LOSS: 0.4712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:18<00:34,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.4402 VAL-LOSS: 0.4487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:21<00:31,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.4242 VAL-LOSS: 0.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:24<00:30,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.4175 VAL-LOSS: 0.4248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:27<00:27,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.4089 VAL-LOSS: 0.4193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:29<00:24,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.3957 VAL-LOSS: 0.4139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:32<00:21,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.3942 VAL-LOSS: 0.4176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:35<00:19,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.3853 VAL-LOSS: 0.4099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:38<00:16,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.3817 VAL-LOSS: 0.3828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:40<00:13,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.3790 VAL-LOSS: 0.3877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:43<00:10,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.3713 VAL-LOSS: 0.3912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:47<00:09,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.3670 VAL-LOSS: 0.3818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:50<00:06,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.3718 VAL-LOSS: 0.3745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:53<00:02,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.3656 VAL-LOSS: 0.3858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:55<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.3634 VAL-LOSS: 0.3787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for 0.5 seconds: 0.2857; for 1.0 second: 0.3545, for 1.5 seconds: 0.5149 Error over all time (training like): 0.3851\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "model = ParticleModelLSTM(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer()\n",
    "TrainingProcedure.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y7mtM7Q4QWPN",
   "metadata": {
    "id": "y7mtM7Q4QWPN"
   },
   "source": [
    "### More LSTM layers (3 layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Aw9xlcRIQZO2",
   "metadata": {
    "id": "Aw9xlcRIQZO2"
   },
   "source": [
    "#### code changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "s4_pQST6QYyl",
   "metadata": {
    "id": "s4_pQST6QYyl"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParticleModelLSTM(torch.nn.Module):\n",
    "    def __init__(self, set_size: int = 5, input_size: int = 5, \n",
    "                 fau1_out: int = 16, gamma1_out: int = 32, fau2_out: int = 32, gamma2_out: int = 32, output_size: int = 2,\n",
    "                 hidden_dim: int = 32, horizon_length: int = 3,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        super(ParticleModelLSTM, self).__init__()\n",
    "        \n",
    "        #### Getting Graph Embedding\n",
    "        \n",
    "        # first iteration \n",
    "        # layer 1\n",
    "        # input target (x_t, y_t, c_t, v_t) + neighbour (x_n, y_n, c_n, v_n) + edge (distance(t, n)) \n",
    "        # as the result input for first layer is x + x + 1 = 2x + 1 (8 + 1 in our case)\n",
    "        self.fau_iteration1 = nn.Sequential(nn.Linear(input_size*2 + 1, fau1_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 2\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_1 \n",
    "        self.gamma_iteration1 = nn.Sequential(nn.Linear(fau1_out+input_size, gamma1_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.embedding_size = gamma1_out\n",
    "\n",
    "        # second iteration \n",
    "        # layer 3\n",
    "        # input target (embedding from layer 1) + neigbour (embedding from layer 1) + edge (distance(t, n))\n",
    "        self.fau_iteration2 = nn.Sequential(nn.Linear(gamma1_out*2 + 1, fau2_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 4\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_3 \n",
    "        self.gamma_iteration2 = nn.Sequential(nn.Linear(fau2_out + gamma1_out, gamma2_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        \n",
    "        #self.final_layer = nn.Sequential(nn.Linear(gamma2_out*set_size, output_size*set_size))\n",
    "\n",
    "         # Recurrent Part\n",
    "        self.rnn = nn.LSTM(gamma2_out*set_size, hidden_dim, num_layers = 3, batch_first = True)\n",
    "        \n",
    "        # Fully connected layer after RNN output, to learn more complex relations\n",
    "        self.final = nn.Linear(hidden_dim, output_size*set_size)\n",
    "\n",
    "\n",
    "        # auxiliary variables \n",
    "        self.output_size = gamma2_out\n",
    "        self.device = device\n",
    "        self.prediction_size = output_size\n",
    "        self.horizon_length = horizon_length\n",
    "        self.set_size = set_size\n",
    "\n",
    "    \n",
    "    def forward_iteration1(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[1], self.embedding_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration1(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate \n",
    "            # x of shape (batch_size, 1, num_features+fau_output)\n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            \n",
    "            embeddings[:,i] = self.gamma_iteration1(x).view(particle_set.shape[0], -1)\n",
    "        # for convenicence, swap axes when return\n",
    "        return embeddings.swapaxes(1,2)\n",
    "\n",
    "    def forward_iteration2(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[2], self.output_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration2(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate the neighborhood and \n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            embeddings[:,i] = self.gamma_iteration2(x).view(particle_set.shape[0], -1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, particle_set):\n",
    "        distances = torch.stack([torch.cdist(x_i, x_i) for x_i in particle_set.swapaxes(1,2)], dim=0).to(self.device)\n",
    "        inner_emedding = self.forward_iteration1(particle_set = particle_set, distances = distances).to(self.device)\n",
    "        embedding = self.forward_iteration2(particle_set = inner_emedding, distances = distances)\n",
    "        zeros = torch.zeros((embedding.shape[0], self.horizon_length, embedding.shape[-1]*self.set_size)).to(self.device) \n",
    "        #(batch_size, 1, embedding_dim)\n",
    "        rnn_input = torch.cat([embedding.view((embedding.shape[0], 1, -1)), zeros], dim=1)\n",
    "        # Input: (Batch size, L (sequence length), input_shape)\n",
    "        # rnn_output: (Batch size, L (sequence length), output_shape)\n",
    "        rnn_output, rnn_state = self.rnn(rnn_input)\n",
    "\n",
    "        output = self.final(rnn_output[:,1:,:])\n",
    "\n",
    "        return output.view((output.shape[0], self.horizon_length, self.prediction_size, self.set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nD_v_TSwQi3k",
   "metadata": {
    "id": "nD_v_TSwQi3k"
   },
   "source": [
    "#### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b-A9_HLsQmhc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-A9_HLsQmhc",
    "outputId": "05a99385-4499-4354-9db6-c9cde1f37383"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:03<01:02,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 2.4154 VAL-LOSS: 1.4600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:06<00:58,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 1.1768 VAL-LOSS: 1.0821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:09<00:54,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.7746 VAL-LOSS: 0.7574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:12<00:50,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.6187 VAL-LOSS: 0.5706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:15<00:47,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.5658 VAL-LOSS: 0.5761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:19<00:44,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.5341 VAL-LOSS: 0.5026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:22<00:41,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.5051 VAL-LOSS: 0.5373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:25<00:37,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.4935 VAL-LOSS: 0.5171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:28<00:34,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.4778 VAL-LOSS: 0.4974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:31<00:31,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.4648 VAL-LOSS: 0.5510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:34<00:28,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.4674 VAL-LOSS: 0.4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:38<00:25,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.4459 VAL-LOSS: 0.4279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:41<00:22,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.4447 VAL-LOSS: 0.4437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:44<00:19,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.4369 VAL-LOSS: 0.4746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:47<00:15,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.4255 VAL-LOSS: 0.4494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:50<00:12,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.4226 VAL-LOSS: 0.4311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:54<00:09,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.4279 VAL-LOSS: 0.4433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:58<00:06,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.4211 VAL-LOSS: 0.4232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [01:01<00:03,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.4219 VAL-LOSS: 0.4370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:04<00:00,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.4153 VAL-LOSS: 0.4147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for 0.5 seconds: 0.3063; for 1.0 second: 0.4012, for 1.5 seconds: 0.5621 Error over all time (training like): 0.4232\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "model = ParticleModelLSTM(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer()\n",
    "TrainingProcedure.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oGXojQo8Q_Qc",
   "metadata": {
    "id": "oGXojQo8Q_Qc"
   },
   "source": [
    "### Adding Embedding Encoding before LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36prjWIeRD4h",
   "metadata": {
    "id": "36prjWIeRD4h"
   },
   "source": [
    "#### code changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "OLyMsRNbQ-wF",
   "metadata": {
    "id": "OLyMsRNbQ-wF"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParticleModelLSTM(torch.nn.Module):\n",
    "    def __init__(self, set_size: int = 5, input_size: int = 5, \n",
    "                 fau1_out: int = 16, gamma1_out: int = 32, fau2_out: int = 32, gamma2_out: int = 32, output_size: int = 2,\n",
    "                 lstm_input: int = 32, hidden_dim: int = 32, horizon_length: int = 3,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        super(ParticleModelLSTM, self).__init__()\n",
    "        \n",
    "        #### Getting Graph Embedding\n",
    "        \n",
    "        # first iteration \n",
    "        # layer 1\n",
    "        # input target (x_t, y_t, c_t, v_t) + neighbour (x_n, y_n, c_n, v_n) + edge (distance(t, n)) \n",
    "        # as the result input for first layer is x + x + 1 = 2x + 1 (8 + 1 in our case)\n",
    "        self.fau_iteration1 = nn.Sequential(nn.Linear(input_size*2 + 1, fau1_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 2\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_1 \n",
    "        self.gamma_iteration1 = nn.Sequential(nn.Linear(fau1_out+input_size, gamma1_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.embedding_size = gamma1_out\n",
    "\n",
    "        # second iteration \n",
    "        # layer 3\n",
    "        # input target (embedding from layer 1) + neigbour (embedding from layer 1) + edge (distance(t, n))\n",
    "        self.fau_iteration2 = nn.Sequential(nn.Linear(gamma1_out*2 + 1, fau2_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 4\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_3 \n",
    "        self.gamma_iteration2 = nn.Sequential(nn.Linear(fau2_out + gamma1_out, gamma2_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        \n",
    "        self.final_embedding_layer = nn.Sequential(nn.Linear(gamma2_out*set_size, lstm_input),\n",
    "                                         nn.LeakyReLU())\n",
    "\n",
    "         # Recurrent Part\n",
    "        self.rnn = nn.LSTM(lstm_input, hidden_dim, num_layers = 1, batch_first = True)\n",
    "        \n",
    "        # Fully connected layer after RNN output, to learn more complex relations\n",
    "        self.final = nn.Linear(hidden_dim, output_size*set_size)\n",
    "\n",
    "\n",
    "        # auxiliary variables \n",
    "        self.output_size = gamma2_out\n",
    "        self.device = device\n",
    "        self.prediction_size = output_size\n",
    "        self.horizon_length = horizon_length\n",
    "        self.set_size = set_size\n",
    "        self.lstm_input = lstm_input\n",
    "\n",
    "    \n",
    "    def forward_iteration1(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[1], self.embedding_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration1(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate \n",
    "            # x of shape (batch_size, 1, num_features+fau_output)\n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            \n",
    "            embeddings[:,i] = self.gamma_iteration1(x).view(particle_set.shape[0], -1)\n",
    "        # for convenicence, swap axes when return\n",
    "        return embeddings.swapaxes(1,2)\n",
    "\n",
    "    def forward_iteration2(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[2], self.output_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration2(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate the neighborhood and \n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            embeddings[:,i] = self.gamma_iteration2(x).view(particle_set.shape[0], -1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, particle_set):\n",
    "        distances = torch.stack([torch.cdist(x_i, x_i) for x_i in particle_set.swapaxes(1,2)], dim=0).to(self.device)\n",
    "        inner_emedding = self.forward_iteration1(particle_set = particle_set, distances = distances).to(self.device)\n",
    "        embedding = self.forward_iteration2(particle_set = inner_emedding, distances = distances)\n",
    "\n",
    "        embedding = self.final_embedding_layer(embedding.view((embedding.shape[0], 1, -1)))\n",
    "\n",
    "        zeros = torch.zeros((embedding.shape[0], self.horizon_length, embedding.shape[-1])).to(self.device) \n",
    "        #(batch_size, 1, embedding_dim)\n",
    "        rnn_input = torch.cat([embedding.view((embedding.shape[0], 1, -1)), zeros], dim=1)\n",
    "        # Input: (Batch size, L (sequence length), input_shape)\n",
    "        # rnn_output: (Batch size, L (sequence length), output_shape)\n",
    "        rnn_output, rnn_state = self.rnn(rnn_input)\n",
    "\n",
    "        output = self.final(rnn_output[:,1:,:])\n",
    "\n",
    "        return output.view((output.shape[0], self.horizon_length, self.prediction_size, self.set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VK3IfeG7RFTV",
   "metadata": {
    "id": "VK3IfeG7RFTV"
   },
   "source": [
    "#### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "vK3fyTGYRGDL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vK3fyTGYRGDL",
    "outputId": "436423a8-177c-45e8-883a-bc3ad625ec25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:04<01:21,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 2.0521 VAL-LOSS: 1.0910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:06<01:00,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.8283 VAL-LOSS: 0.6878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:09<00:51,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.6365 VAL-LOSS: 0.6558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:12<00:46,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.5706 VAL-LOSS: 0.5278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:14<00:42,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.5310 VAL-LOSS: 0.5090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:17<00:38,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.5010 VAL-LOSS: 0.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:20<00:35,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.4903 VAL-LOSS: 0.4846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:23<00:32,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.4681 VAL-LOSS: 0.4669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:25<00:29,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.4726 VAL-LOSS: 0.4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:28<00:27,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.4559 VAL-LOSS: 0.4423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:31<00:24,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.4494 VAL-LOSS: 0.4462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:33<00:21,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.4440 VAL-LOSS: 0.4716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:36<00:18,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.4508 VAL-LOSS: 0.4472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:39<00:16,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.4266 VAL-LOSS: 0.4403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:41<00:13,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.4286 VAL-LOSS: 0.4398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:44<00:10,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.4233 VAL-LOSS: 0.4215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:47<00:08,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.4230 VAL-LOSS: 0.4562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:49<00:05,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.4231 VAL-LOSS: 0.4439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:52<00:02,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.4178 VAL-LOSS: 0.4289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:55<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.4099 VAL-LOSS: 0.3971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for 0.5 seconds: 0.2828; for 1.0 second: 0.3716, for 1.5 seconds: 0.5452 Error over all time (training like): 0.3999\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "model = ParticleModelLSTM(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer()\n",
    "TrainingProcedure.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eiMjFFLbR0CE",
   "metadata": {
    "id": "eiMjFFLbR0CE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "faec2056",
    "059b633c",
    "18b2874d",
    "dea70d73",
    "O3jtlEzZEwTE",
    "b5cf8f64",
    "qt1bd2y9C760",
    "w3kzTcgQHHdg",
    "o09IWs0mKmaA",
    "I_Z-Jfm4M1gy",
    "-KQsWYrUP6hm",
    "T_QraFA7P_G3",
    "s0EbjfQ0QDFs",
    "y7mtM7Q4QWPN",
    "Aw9xlcRIQZO2",
    "36prjWIeRD4h",
    "VK3IfeG7RFTV"
   ],
   "name": "Assignment_2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "b001d610a241339cc3b7988a7f6c804c70cb4dbbf032519cbfea0d67797e8b2b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
