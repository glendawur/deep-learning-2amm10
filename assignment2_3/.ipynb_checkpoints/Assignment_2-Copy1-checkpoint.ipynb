{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32f8d18",
   "metadata": {
    "id": "d32f8d18"
   },
   "source": [
    "# Group Number:\n",
    "\n",
    "# Student 1: Ryan Meghoe\n",
    "\n",
    "# Student 2: Nikita Jain\n",
    "\n",
    "# Student 3: Andrei Rykov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec2056",
   "metadata": {
    "id": "faec2056"
   },
   "source": [
    "# Downloading Data and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d0580a5",
   "metadata": {
    "id": "7d0580a5"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0756591",
   "metadata": {
    "id": "b0756591"
   },
   "outputs": [],
   "source": [
    "def load_zip(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipf = ZipFile(io.BytesIO(response.content))\n",
    "    return {name: zipf.read(name) for name in zipf.namelist()}\n",
    "\n",
    "def load_array(zipfile, fn):\n",
    "    return np.load(io.BytesIO(zipfile[fn]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb77a4be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb77a4be",
    "outputId": "709f5ba3-89e2-4d00-dec3-a61c777c4e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the training data:\n",
      "\n",
      "positions: (10000, 4, 2, 5)\n",
      "velocities: (10000, 1, 2, 5)\n",
      "charges: (10000, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell loads the training, validation or test data as numpy arrays,\n",
    "with the positions, initial velocities and charge data of the particles.\n",
    "\n",
    "The position arrays are shaped as\n",
    "[simulation id, time point (corresponding to t = 0, 0.5, 1 or 1.5), x/y spatial dimension, particle id].\n",
    "\n",
    "The initial velocity arrays are shaped as\n",
    "[simulation id, 1 (corresponding to t=0), x/y spatial dimension, particle id].\n",
    "\n",
    "The charge arrays are shaped as [simulation id, particle id, 1]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data = load_zip('https://surfdrive.surf.nl/files/index.php/s/OIgda2ZRG8v0eqB/download')\n",
    "\n",
    "features = ['positions', 'velocities', 'charges']\n",
    "    \n",
    "positions_train, velocities_train, charges_train = (load_array(data, f'data/train/{f}.npy') for f in features)\n",
    "positions_valid, velocities_valid, charges_valid = (load_array(data, f'data/valid/{f}.npy') for f in features)\n",
    "positions_test, velocities_test, charges_test = (load_array(data, f'data/test/{f}.npy') for f in features)\n",
    "\n",
    "print('Shapes of the training data:\\n')\n",
    "print(f'positions: {positions_train.shape}')\n",
    "print(f'velocities: {velocities_train.shape}')\n",
    "print(f'charges: {charges_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3ea4cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c3ea4cb",
    "outputId": "276125ce-2246-4ff4-e8fa-0befa892ceca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of retrieving data from the arrays:\n",
      "\n",
      "\n",
      "In simulation 42 of the training set, particle 3 with charge -1.0 had coordinates [ 2.05159559 -1.46130851].\n",
      "The initial velocity of this particle was [ 0.28402364 -0.24784824].\n"
     ]
    }
   ],
   "source": [
    "print('An example of retrieving data from the arrays:\\n\\n')\n",
    "\n",
    "sim_idx = 42\n",
    "t_idx = 2  # t_idx 0, 1, 2, 3 corresponds to t=0, 0.5, 1 and 1.5 respectively\n",
    "spatial_idx = (0,1)  # corresponds to both x and y dimension\n",
    "particle_idx = 3  # corresponds to particle with index 3\n",
    "\n",
    "p = positions_train[sim_idx, t_idx, spatial_idx, particle_idx]\n",
    "v = velocities_train[sim_idx, 0, spatial_idx, particle_idx]  # note: this array contains only the inital velocity -> hence the 0\n",
    "c = charges_train[sim_idx, particle_idx, 0] \n",
    "\n",
    "print(\n",
    "    f'In simulation {sim_idx} of the training set, particle {particle_idx} with charge {c} had coordinates {p}.\\nThe initial velocity of this particle was {v}.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a3438a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10a3438a",
    "outputId": "9bb70b84-c8ef-4a49-f4b0-6a8af182084d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of no. datapoints:\n",
      "\n",
      "10000 train, 2000 validation, 2000 test simulations\n"
     ]
    }
   ],
   "source": [
    "print('Overview of no. datapoints:\\n')\n",
    "\n",
    "print(f'{len(positions_train)} train, {len(positions_valid)} validation, {len(positions_test)} test simulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9106543",
   "metadata": {
    "id": "f9106543"
   },
   "outputs": [],
   "source": [
    "def plot_example(pos, vel):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-5., 5.])\n",
    "    axes.set_ylim([-5., 5.])\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'brown']\n",
    "    for i in range(pos.shape[-1]):\n",
    "        plt.plot(pos[0, 0, i], pos[0, 1, i], 'd', color=colors[i])\n",
    "        plt.plot(pos[-1, 0, i], pos[-1, 1, i], 'x', color=colors[i])\n",
    "        plt.plot([pos[0, 0, i], pos[0, 0, i] + vel[0, 0, i]], [pos[0, 1, i], pos[0, 1, i] + vel[0, 1, i]], '--', color=colors[i])\n",
    "    fig.set_size_inches(7, 7)\n",
    "    plt.xlim(np.min(pos)-1, np.max(pos) +1)\n",
    "    plt.ylim(np.min(pos)-1, np.max(pos) +1)\n",
    "    plt.plot([], [], 'd', color='black', label='initial position')\n",
    "    plt.plot([], [], 'x', color='black', label='final position')\n",
    "    plt.plot([], [], '--', color='black', label='initial velocity \\ndirection and magnitude')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d28681a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "d28681a6",
    "outputId": "b071f957-4e72-4f47-bc8f-797738fad547"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_idx = np.random.randint(0, 10000)\n",
    "plot_example(positions_train[random_idx], velocities_train[random_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b633c",
   "metadata": {
    "id": "059b633c"
   },
   "source": [
    "# Data Handling and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6ecb529",
   "metadata": {
    "id": "e6ecb529"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train = torch.cat((torch.tensor(positions_train[:,0,:,:]), torch.tensor(charges_train).squeeze(-1).unsqueeze(1)), dim=1)\n",
    "X_train = torch.cat((X_train, torch.tensor(velocities_train).squeeze(1)), dim=1) # shape: (simulation id, parameters (x, y, c, v_x, v_y), particle id)\n",
    "y_train = torch.tensor(positions_train[:,1:,:,:]) # shape: (simulation id, time (0.5, 1, 1.5), (x, y), particle id)\n",
    "\n",
    "X_valid = torch.cat((torch.tensor(positions_valid[:,0,:,:]), torch.tensor(charges_valid).squeeze(-1).unsqueeze(1)), dim=1)\n",
    "X_valid = torch.cat((X_valid, torch.tensor(velocities_valid).squeeze(1)), dim=1)\n",
    "y_valid = torch.tensor(positions_valid[:,1:,:,:])\n",
    "\n",
    "X_test = torch.cat((torch.tensor(positions_test[:,0,:,:]), torch.tensor(charges_test).squeeze(-1).unsqueeze(1)), dim=1)\n",
    "X_test = torch.cat((X_test, torch.tensor(velocities_test).squeeze(1)), dim=1)\n",
    "y_test = torch.tensor(positions_test[:,1:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8633eb8",
   "metadata": {
    "id": "f8633eb8"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a99a32b",
   "metadata": {
    "id": "0a99a32b"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blkK-lVfx0SK",
   "metadata": {
    "id": "blkK-lVfx0SK"
   },
   "source": [
    "# Model 1\n",
    "\n",
    "The GraphNN-like model that makes predictions based on the embedding of the set. For each timestamp we train a separate model in this section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2874d",
   "metadata": {
    "id": "18b2874d"
   },
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66774050",
   "metadata": {
    "id": "66774050"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParticleModel(torch.nn.Module):\n",
    "    def __init__(self, set_size: int = 5, input_size: int = 5, \n",
    "                 fau1_out: int = 16, gamma1_out: int = 32, fau2_out: int = 32, gamma2_out: int = 32, output_size: int = 2, device: torch.device = torch.device('cpu')):\n",
    "        super(ParticleModel, self).__init__()\n",
    "        \n",
    "        #### Getting Graph Embedding\n",
    "        \n",
    "        # first iteration \n",
    "        # layer 1\n",
    "        # input target (x_t, y_t, c_t, v_t) + neighbour (x_n, y_n, c_n, v_n) + edge (distance(t, n)) \n",
    "        # as the result input for first layer is x + x + 1 = 2x + 1 (8 + 1 in our case)\n",
    "        self.fau_iteration1 = nn.Sequential(nn.Linear(input_size*2 + 1, fau1_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 2\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_1 \n",
    "        self.gamma_iteration1 = nn.Sequential(nn.Linear(fau1_out+input_size, gamma1_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.embedding_size = gamma1_out\n",
    "\n",
    "        # second iteration \n",
    "        # layer 3\n",
    "        # input target (embedding from layer 1) + neigbour (embedding from layer 1) + edge (distance(t, n))\n",
    "        self.fau_iteration2 = nn.Sequential(nn.Linear(gamma1_out*2 + 1, fau2_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 4\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_3 \n",
    "        self.gamma_iteration2 = nn.Sequential(nn.Linear(fau2_out + gamma1_out, gamma2_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.output_size = gamma2_out\n",
    "        self.device = device\n",
    "        self.prediction_size = output_size\n",
    "        self.final_layer = nn.Sequential(nn.Linear(gamma2_out, 64),\n",
    "                                         nn.LeakyReLU(),\n",
    "                                         #nn.Dropout(p=0.5),\n",
    "                                         #nn.BatchNorm1d(1028),\n",
    "                                         nn.Linear(64, output_size))\n",
    "\n",
    "    \n",
    "    def forward_iteration1(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[1], self.embedding_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration1(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate \n",
    "            # x of shape (batch_size, 1, num_features+fau_output)\n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            \n",
    "            embeddings[:,i] = self.gamma_iteration1(x).view(particle_set.shape[0], -1)\n",
    "        # for convenicence, swap axes when return\n",
    "        return embeddings.swapaxes(1,2)\n",
    "\n",
    "    def forward_iteration2(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[2], self.output_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration2(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate the neighborhood and \n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            embeddings[:,i] = self.gamma_iteration2(x).view(particle_set.shape[0], -1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, particle_set):\n",
    "        distances = torch.stack([torch.cdist(x_i, x_i) for x_i in particle_set.swapaxes(1,2)], dim=0).to(self.device)\n",
    "        inner_emedding = self.forward_iteration1(particle_set = particle_set, distances = distances).to(self.device)\n",
    "        embedding = self.forward_iteration2(particle_set = inner_emedding, distances = distances)\n",
    "        \n",
    "        prediction = self.final_layer(embedding)\n",
    "        #print(prediction.shape)\n",
    "        return prediction.view(embedding.shape[0], self.prediction_size, -1) # (particle_set.shape[0], 2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea70d73",
   "metadata": {
    "id": "dea70d73"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e95af5f9",
   "metadata": {
    "id": "e95af5f9"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class ParticleDistanceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "      The loss to calculate mean distance between predicted location of particle of each set.\n",
    "      By defaul, Euclidean distance is set\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, norm: float = 2):\n",
    "        super(ParticleDistanceLoss, self).__init__()\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, input_set, target_set):\n",
    "        \"\"\"\n",
    "            (batch_size, coordinates, set_size)\n",
    "        \"\"\"\n",
    "        return (input_set - target_set).norm(p = self.norm, dim = 1).mean(axis = 1).mean()\n",
    "        \n",
    "        \n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_DataLoader: torch.utils.data.Dataset,\n",
    "                 validation_DataLoader: torch.utils.data.Dataset ,\n",
    "                 testing_DataLoader: torch.utils.data.Dataset ,\n",
    "                 epochs: int\n",
    "                 ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.training_DataLoader = training_DataLoader\n",
    "        self.validation_DataLoader = validation_DataLoader\n",
    "        self.testing_DataLoader = testing_DataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "    def run_trainer(self, target_time):\n",
    "\n",
    "        epoch_train_losses = []\n",
    "        epoch_val_losses = []\n",
    "\n",
    "        times = [0.5, 1, 1.5]\n",
    "        time_to_ind = {0.5:0, 1:1, 1.5:2}\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "                 \n",
    "\n",
    "            self.model.train()  # train mode\n",
    "\n",
    "            train_losses=[]\n",
    "            \n",
    "            correct = 0\n",
    "            length = 0\n",
    "            for x, y in self.training_DataLoader:\n",
    "\n",
    "                A, B = x.float().to(self.device), y.float().to(self.device) # send to device (GPU or CPU)\n",
    "\n",
    "                self.optimizer.zero_grad()  # zerograd the parameters\n",
    "\n",
    "                loss = 0\n",
    "                out = self.model(A)  # one forward pass\n",
    "                print(B[:,time_to_ind[target_time]].shape)\n",
    "                loss += self.criterion(out, B[:,time_to_ind[target_time]])  # calculate loss\n",
    "                \n",
    "                loss_value = loss.item()\n",
    "                train_losses.append(loss_value)\n",
    "                 \n",
    "                loss.backward()  # one backward pass\n",
    "                self.optimizer.step()  # update the parameters\n",
    "            \n",
    "            epoch_train_losses.append(np.mean(train_losses))\n",
    "            self.model.eval()  # evaluation mode\n",
    "            valid_losses = []  # accumulate the losses here\n",
    "\n",
    "            correct = 0\n",
    "            length = 0\n",
    "            for x,  y in self.validation_DataLoader:\n",
    "\n",
    "                A,B = x.float().to(self.device), y.float().to(self.device) # send to device (GPU or CPU)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    loss = 0\n",
    "                    \n",
    "                    out = self.model(A)  # one forward pass\n",
    "                    loss += self.criterion(out, B[:,time_to_ind[target_time]])  # calculate loss\n",
    "                 \n",
    "                    loss_value = loss.item()\n",
    "                    valid_losses.append(loss_value)\n",
    "\n",
    "            epoch_val_losses.append(np.mean(valid_losses))\n",
    "                \n",
    "            # print the results\n",
    "            print(\n",
    "                f'EPOCH: {epoch+1:0>{len(str(self.epochs))}}/{self.epochs}',\n",
    "                end=' '\n",
    "            )\n",
    "            print(f'LOSS: {np.mean(train_losses):.4f}',end=' ')\n",
    "            print(f'VAL-LOSS: {np.mean(valid_losses):.4f}',end='\\n')\n",
    "\n",
    "        return epoch_train_losses, epoch_val_losses\n",
    "        \n",
    "    def evaluate(self, target_time):\n",
    "\n",
    "        self.model.eval()\n",
    "        times = [0.5, 1, 1.5]\n",
    "        time_to_ind = {0.5:0, 1:1, 1.5:2}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss = []\n",
    "            length = 0\n",
    "            for x,y in self.testing_DataLoader:\n",
    "                \n",
    "                A, B = x.float().to(self.device), y.float().to(self.device)\n",
    "                out = self.model(A)\n",
    "                loss.append(self.criterion(out, B[:,time_to_ind[target_time]]))\n",
    "        print(f'Error: {np.mean(loss):.4f} for target time {target_time}',end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bPfkmpEqnT",
   "metadata": {
    "id": "d9bPfkmpEqnT"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O3jtlEzZEwTE",
   "metadata": {
    "id": "O3jtlEzZEwTE"
   },
   "source": [
    "### Training with max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07e03ddf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "07e03ddf",
    "outputId": "53ff3afb-6f7a-435a-ccbf-306999bf00e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/20 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n",
      "torch.Size([64, 2, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ONII-C~1\\AppData\\Local\\Temp/ipykernel_23792/2841780271.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                             epochs = 20)\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainingProcedure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mTrainingProcedure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ONII-C~1\\AppData\\Local\\Temp/ipykernel_23792/958459129.py\u001b[0m in \u001b[0;36mrun_trainer\u001b[1;34m(self, target_time)\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# one backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# update the parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "model = ParticleModel(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer(target_time=0.5)\n",
    "TrainingProcedure.evaluate(target_time=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0f24a09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0f24a09",
    "outputId": "772c9a7d-7100-4693-a46b-cdcec564f862"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ONII-C~1\\AppData\\Local\\Temp/ipykernel_23792/959176161.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                             epochs = 20)\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainingProcedure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mTrainingProcedure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ONII-C~1\\AppData\\Local\\Temp/ipykernel_23792/3994319407.py\u001b[0m in \u001b[0;36mrun_trainer\u001b[1;34m(self, target_time)\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# one backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# update the parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ParticleModel(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer(target_time=1)\n",
    "TrainingProcedure.evaluate(target_time=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f81969",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4f81969",
    "outputId": "590b4283-886f-477e-8818-9b838c1fbea2"
   },
   "outputs": [],
   "source": [
    "model = ParticleModel(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer(target_time=1.5)\n",
    "TrainingProcedure.evaluate(target_time=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf8f64",
   "metadata": {
    "id": "b5cf8f64"
   },
   "source": [
    "### Linear interpolation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c619cd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c619cd3",
    "outputId": "c3d56479-27a9-4e0f-c7f2-8440cffeec66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of mean distance (Eudclidean) for 0.5 seconds: 0.1682; for 1.0 second: 0.3932, for 1.5 seconds: 0.6314 "
     ]
    }
   ],
   "source": [
    "# (simulation id, time (0.5, 1, 1.5), (x, y), particle id)\n",
    "# (simulation id, parameters (x, y, c, v_x, v_y), particle id)\n",
    "\n",
    "def predict(x, time):\n",
    "    predictions = torch.cat([x[:,0,:]+ time*x[:,-2,:], x[:,1,:] + time*x[:,-1,:]], dim = 1)\n",
    "    return predictions.view((x.shape[0], -1, x.shape[-1]))\n",
    "\n",
    "loss_f = ParticleDistanceLoss(norm = 2)\n",
    "\n",
    "times = [0.5, 1, 1.5]\n",
    "time_to_ind = {0.5:0, 1:1, 1.5:2}\n",
    "loss = [[],[],[]]\n",
    "for x,y in test_dataloader:\n",
    "    for time in times:                    \n",
    "        A, B = x.float(), y.float()\n",
    "        out = predict(A, time)\n",
    "        loss[time_to_ind[time]].append(loss_f(out, B[:,time_to_ind[time]]))\n",
    "\n",
    "print(f'Mean of mean distance (Eudclidean) for 0.5 seconds: {np.mean(loss[0]):.4f}; for 1.0 second: {np.mean(loss[1]):.4f}, for 1.5 seconds: {np.mean(loss[2]):.4f}',end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qt1bd2y9C760",
   "metadata": {
    "id": "qt1bd2y9C760"
   },
   "source": [
    "### Training with Mean Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "Kc9LFwZICSx1",
   "metadata": {
    "id": "Kc9LFwZICSx1"
   },
   "outputs": [],
   "source": [
    "class ParticleModel(torch.nn.Module):\n",
    "    def __init__(self, set_size: int = 5, input_size: int = 5, \n",
    "                 fau1_out: int = 16, gamma1_out: int = 32, fau2_out: int = 32, gamma2_out: int = 32, output_size: int = 2, device: torch.device = torch.device('cpu')):\n",
    "        super(ParticleModel, self).__init__()\n",
    "        \n",
    "        #### Getting Graph Embedding\n",
    "        \n",
    "        # first iteration \n",
    "        # layer 1\n",
    "        # input target (x_t, y_t, c_t, v_t) + neighbour (x_n, y_n, c_n, v_n) + edge (distance(t, n)) \n",
    "        # as the result input for first layer is x + x + 1 = 2x + 1 (8 + 1 in our case)\n",
    "        self.fau_iteration1 = nn.Sequential(nn.Linear(input_size*2 + 1, fau1_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 2\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_1 \n",
    "        self.gamma_iteration1 = nn.Sequential(nn.Linear(fau1_out+input_size, gamma1_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.embedding_size = gamma1_out\n",
    "\n",
    "        # second iteration \n",
    "        # layer 3\n",
    "        # input target (embedding from layer 1) + neigbour (embedding from layer 1) + edge (distance(t, n))\n",
    "        self.fau_iteration2 = nn.Sequential(nn.Linear(gamma1_out*2 + 1, fau2_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 4\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_3 \n",
    "        self.gamma_iteration2 = nn.Sequential(nn.Linear(fau2_out + gamma1_out, gamma2_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.output_size = gamma2_out\n",
    "        self.device = device\n",
    "        self.prediction_size = output_size\n",
    "        self.final_layer = nn.Sequential(nn.Linear(gamma2_out*set_size, output_size*set_size))\n",
    "\n",
    "    \n",
    "    def forward_iteration1(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[1], self.embedding_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration1(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.mean(axis = 1)\n",
    "\n",
    "            # concatenate \n",
    "            # x of shape (batch_size, 1, num_features+fau_output)\n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            \n",
    "            embeddings[:,i] = self.gamma_iteration1(x).view(particle_set.shape[0], -1)\n",
    "        # for convenicence, swap axes when return\n",
    "        return embeddings.swapaxes(1,2)\n",
    "\n",
    "    def forward_iteration2(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[2], self.output_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration2(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.mean(axis = 1)\n",
    "\n",
    "            # concatenate the neighborhood and \n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            embeddings[:,i] = self.gamma_iteration2(x).view(particle_set.shape[0], -1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, particle_set):\n",
    "        distances = torch.stack([torch.cdist(x_i, x_i) for x_i in particle_set.swapaxes(1,2)], dim=0).to(self.device)\n",
    "        inner_emedding = self.forward_iteration1(particle_set = particle_set, distances = distances).to(self.device)\n",
    "        embedding = self.forward_iteration2(particle_set = inner_emedding, distances = distances)\n",
    "        \n",
    "        prediction = self.final_layer(embedding.view(embedding.shape[0], -1))\n",
    "\n",
    "        return prediction.view(embedding.shape[0], self.prediction_size, -1) # (particle_set.shape[0], 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ZwZsl0pDChJt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwZsl0pDChJt",
    "outputId": "594870eb-edbc-43f2-fafb-2cca22bf59f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:02<00:38,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 0.5408 VAL-LOSS: 0.2548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:04<00:36,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.2278 VAL-LOSS: 0.2168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:05<00:33,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.2030 VAL-LOSS: 0.1901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:07<00:31,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.1887 VAL-LOSS: 0.1950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:09<00:29,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.1837 VAL-LOSS: 0.1716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:11<00:27,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.1760 VAL-LOSS: 0.1985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:13<00:25,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.1757 VAL-LOSS: 0.1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:15<00:23,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.1736 VAL-LOSS: 0.1863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:17<00:21,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.1705 VAL-LOSS: 0.1723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:19<00:19,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.1696 VAL-LOSS: 0.1871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:21<00:17,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.1709 VAL-LOSS: 0.1595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:23<00:16,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.1663 VAL-LOSS: 0.1837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:25<00:13,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.1679 VAL-LOSS: 0.1836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:27<00:11,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.1649 VAL-LOSS: 0.1563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:29<00:09,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.1648 VAL-LOSS: 0.1630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:31<00:07,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.1648 VAL-LOSS: 0.1704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:33<00:05,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.1651 VAL-LOSS: 0.1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:35<00:03,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.1623 VAL-LOSS: 0.1781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:37<00:01,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.1608 VAL-LOSS: 0.1519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:39<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.1569 VAL-LOSS: 0.1558\n",
      "Error: 0.1591 for target time 0.5 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:02<00:38,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 0.7293 VAL-LOSS: 0.4300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:04<00:36,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.4023 VAL-LOSS: 0.3737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:06<00:34,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.3513 VAL-LOSS: 0.3524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:08<00:32,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.3376 VAL-LOSS: 0.3355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:10<00:29,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.3318 VAL-LOSS: 0.3349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:12<00:27,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.3248 VAL-LOSS: 0.3191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:14<00:26,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.3146 VAL-LOSS: 0.3214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:16<00:24,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.3145 VAL-LOSS: 0.3186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:18<00:22,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.3098 VAL-LOSS: 0.3069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:20<00:19,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.3082 VAL-LOSS: 0.3103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:22<00:17,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.3022 VAL-LOSS: 0.3212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:24<00:15,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.3045 VAL-LOSS: 0.2877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:26<00:14,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.2994 VAL-LOSS: 0.3045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:28<00:12,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.2956 VAL-LOSS: 0.2915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:30<00:10,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.2892 VAL-LOSS: 0.2936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:32<00:08,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.2885 VAL-LOSS: 0.2819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:34<00:06,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.2859 VAL-LOSS: 0.2871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:36<00:04,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.2799 VAL-LOSS: 0.2794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:38<00:02,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.2772 VAL-LOSS: 0.2846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:40<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.2737 VAL-LOSS: 0.2693\n",
      "Error: 0.2766 for target time 1 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:01<00:37,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 0.9329 VAL-LOSS: 0.5707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:03<00:35,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.5407 VAL-LOSS: 0.5211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:05<00:33,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.5021 VAL-LOSS: 0.5183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:07<00:31,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.4818 VAL-LOSS: 0.4719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:09<00:29,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.4746 VAL-LOSS: 0.4573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:11<00:27,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.4638 VAL-LOSS: 0.4763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:13<00:25,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.4514 VAL-LOSS: 0.4665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:15<00:23,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.4447 VAL-LOSS: 0.4427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:17<00:21,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.4385 VAL-LOSS: 0.4313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:19<00:19,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.4277 VAL-LOSS: 0.4697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:21<00:17,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.4284 VAL-LOSS: 0.4302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:23<00:15,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.4173 VAL-LOSS: 0.4454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:25<00:13,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.4152 VAL-LOSS: 0.4289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:27<00:11,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.4057 VAL-LOSS: 0.4157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:29<00:09,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.4002 VAL-LOSS: 0.4038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:31<00:07,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.3958 VAL-LOSS: 0.3947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:33<00:05,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.3896 VAL-LOSS: 0.3925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:35<00:03,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.3820 VAL-LOSS: 0.4133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:37<00:01,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.3781 VAL-LOSS: 0.3833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:39<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.3773 VAL-LOSS: 0.3861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.3930 for target time 1.5 "
     ]
    }
   ],
   "source": [
    "times = [0.5, 1, 1.5]\n",
    "time_to_ind = {0.5:0, 1:1, 1.5:2}\n",
    "\n",
    "for time in times:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device=torch.device('cpu')\n",
    "\n",
    "    model = ParticleModel(device = device).to(device)\n",
    "    criterion = ParticleDistanceLoss(norm=2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    TrainingProcedure = Trainer(model, \n",
    "                                device, \n",
    "                                criterion, \n",
    "                                optimizer,\n",
    "                                train_dataloader,\n",
    "                                valid_dataloader,\n",
    "                                test_dataloader,\n",
    "                                epochs = 20)\n",
    "\n",
    "    train_loss, val_loss = TrainingProcedure.run_trainer(target_time=time)\n",
    "    TrainingProcedure.evaluate(target_time=time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GEZhrF4wEDI0",
   "metadata": {
    "id": "GEZhrF4wEDI0"
   },
   "source": [
    "### Results comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZVQNHIGCD0ME",
   "metadata": {
    "id": "ZVQNHIGCD0ME"
   },
   "source": [
    "#### Comparison\n",
    "\n",
    "**Mean pooling**:\n",
    "\n",
    "Mean distance (Eudclidean) for 0.5 seconds: 0.1591; for 1.0 second: 0.2766, for 1.5 seconds: 0.3930\n",
    "\n",
    "**Max pooling**: \n",
    "\n",
    "Mean distance (Eudclidean) for 0.5 seconds: 0.1558; for 1.0 second: 0.2641, for 1.5 seconds: 0.3471\n",
    "\n",
    "**Linear interpolation**:\n",
    "\n",
    "Mean distance (Eudclidean) for 0.5 seconds: 0.1682; for 1.0 second: 0.3932, for 1.5 seconds: 0.6314 \n",
    "\n",
    "As the result, simple model provides more precise prediction as the time goe than a linear interpolation by the formula $ x^t_i = x^0_i + v^0_i*t $, where $x$ is the coordinate and $v$ is velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12U9ShwSGi9a",
   "metadata": {
    "id": "12U9ShwSGi9a"
   },
   "source": [
    "# Model 2\n",
    "\n",
    "The model with GraphNN-like embedding extraction followed by the LSTM. That model is considered to make prediction based on previous prediction and thus creates sequence of predictions for next **n** timestamps with step-size of 0.5 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w3kzTcgQHHdg",
   "metadata": {
    "id": "w3kzTcgQHHdg"
   },
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8UP7RPOBG7R8",
   "metadata": {
    "id": "8UP7RPOBG7R8"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParticleModelLSTM(torch.nn.Module):\n",
    "    def __init__(self, set_size: int = 5, input_size: int = 5, \n",
    "                 fau1_out: int = 16, gamma1_out: int = 32, fau2_out: int = 32, gamma2_out: int = 32, output_size: int = 2,\n",
    "                 hidden_dim: int = 32, horizon_length: int = 3,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        super(ParticleModelLSTM, self).__init__()\n",
    "        \n",
    "        #### Getting Graph Embedding\n",
    "        \n",
    "        # first iteration \n",
    "        # layer 1\n",
    "        # input target (x_t, y_t, c_t, v_t) + neighbour (x_n, y_n, c_n, v_n) + edge (distance(t, n)) \n",
    "        # as the result input for first layer is x + x + 1 = 2x + 1 (8 + 1 in our case)\n",
    "        self.fau_iteration1 = nn.Sequential(nn.Linear(input_size*2 + 1, fau1_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 2\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_1 \n",
    "        self.gamma_iteration1 = nn.Sequential(nn.Linear(fau1_out+input_size, gamma1_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.embedding_size = gamma1_out\n",
    "\n",
    "        # second iteration \n",
    "        # layer 3\n",
    "        # input target (embedding from layer 1) + neigbour (embedding from layer 1) + edge (distance(t, n))\n",
    "        self.fau_iteration2 = nn.Sequential(nn.Linear(gamma1_out*2 + 1, fau2_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 4\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_3 \n",
    "        self.gamma_iteration2 = nn.Sequential(nn.Linear(fau2_out + gamma1_out, gamma2_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        \n",
    "        #self.final_layer = nn.Sequential(nn.Linear(gamma2_out*set_size, output_size*set_size))\n",
    "\n",
    "         # Recurrent Part\n",
    "        self.rnn = nn.LSTM(gamma2_out*set_size, hidden_dim, num_layers = 1, batch_first = True)\n",
    "        \n",
    "        # Fully connected layer after RNN output, to learn more complex relations\n",
    "        self.final = nn.Linear(hidden_dim, output_size*set_size)\n",
    "\n",
    "\n",
    "        # auxiliary variables \n",
    "        self.output_size = gamma2_out\n",
    "        self.device = device\n",
    "        self.prediction_size = output_size\n",
    "        self.horizon_length = horizon_length\n",
    "        self.set_size = set_size\n",
    "\n",
    "    \n",
    "    def forward_iteration1(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[1], self.embedding_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration1(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate \n",
    "            # x of shape (batch_size, 1, num_features+fau_output)\n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            \n",
    "            embeddings[:,i] = self.gamma_iteration1(x).view(particle_set.shape[0], -1)\n",
    "        # for convenicence, swap axes when return\n",
    "        return embeddings.swapaxes(1,2)\n",
    "\n",
    "    def forward_iteration2(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[2], self.output_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration2(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate the neighborhood and \n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            embeddings[:,i] = self.gamma_iteration2(x).view(particle_set.shape[0], -1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, particle_set):\n",
    "        distances = torch.stack([torch.cdist(x_i, x_i) for x_i in particle_set.swapaxes(1,2)], dim=0).to(self.device)\n",
    "        inner_emedding = self.forward_iteration1(particle_set = particle_set, distances = distances).to(self.device)\n",
    "        embedding = self.forward_iteration2(particle_set = inner_emedding, distances = distances)\n",
    "        zeros = torch.zeros((embedding.shape[0], self.horizon_length, embedding.shape[-1]*self.set_size)).to(self.device) \n",
    "        #(batch_size, 1, embedding_dim)\n",
    "        rnn_input = torch.cat([embedding.view((embedding.shape[0], 1, -1)), zeros], dim=1)\n",
    "        # Input: (Batch size, L (sequence length), input_shape)\n",
    "        # rnn_output: (Batch size, L (sequence length), output_shape)\n",
    "        rnn_output, rnn_state = self.rnn(rnn_input)\n",
    "\n",
    "        output = self.final(rnn_output[:,1:,:])\n",
    "\n",
    "        return output.view((output.shape[0], self.horizon_length, self.prediction_size, self.set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o09IWs0mKmaA",
   "metadata": {
    "id": "o09IWs0mKmaA"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fx-o9sRWIl0d",
   "metadata": {
    "id": "fx-o9sRWIl0d"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_DataLoader: torch.utils.data.Dataset,\n",
    "                 validation_DataLoader: torch.utils.data.Dataset ,\n",
    "                 testing_DataLoader: torch.utils.data.Dataset ,\n",
    "                 epochs: int\n",
    "                 ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.training_DataLoader = training_DataLoader\n",
    "        self.validation_DataLoader = validation_DataLoader\n",
    "        self.testing_DataLoader = testing_DataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "    def run_trainer(self):\n",
    "\n",
    "        epoch_train_losses = []\n",
    "        epoch_val_losses = []\n",
    "\n",
    "        times = [0.5, 1, 1.5]\n",
    "        time_to_ind = {0.5:0, 1:1, 1.5:2}\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "                 \n",
    "\n",
    "            self.model.train()  # train mode\n",
    "\n",
    "            train_losses=[]\n",
    "            \n",
    "            for x, y in self.training_DataLoader:\n",
    "\n",
    "                x, y = x.float().to(self.device), y.float().to(self.device) # send to device (GPU or CPU)\n",
    "\n",
    "                self.optimizer.zero_grad()  # zerograd the parameters\n",
    "\n",
    "                out = self.model(x)\n",
    "                loss = 0\n",
    "                for i in range(3):\n",
    "                    loss += self.criterion(y[:,i], out[:,i])\n",
    "                \n",
    "                loss_value = loss.item()/3\n",
    "                train_losses.append(loss_value)\n",
    "                 \n",
    "                loss.backward()  # one backward pass\n",
    "                self.optimizer.step()  # update the parameters\n",
    "            \n",
    "            epoch_train_losses.append(np.mean(train_losses))\n",
    "            self.model.eval()  # evaluation mode\n",
    "            valid_losses = []  # accumulate the losses here\n",
    "\n",
    "            correct = 0\n",
    "            length = 0\n",
    "            for x,  y in self.validation_DataLoader:\n",
    "\n",
    "                x,y = x.float().to(self.device), y.float().to(self.device) # send to device (GPU or CPU)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    out = self.model(x)\n",
    "                    loss = 0\n",
    "                    for i in range(3):\n",
    "                        loss += self.criterion(y[:,i], out[:,i])\n",
    "                    \n",
    "                    loss_value = loss.item()/3\n",
    "                    valid_losses.append(loss_value)\n",
    "\n",
    "            epoch_val_losses.append(np.mean(valid_losses))\n",
    "                \n",
    "            # print the results\n",
    "            print(\n",
    "                f'EPOCH: {epoch+1:0>{len(str(self.epochs))}}/{self.epochs}',\n",
    "                end=' '\n",
    "            )\n",
    "            print(f'LOSS: {np.mean(train_losses):.4f}',end=' ')\n",
    "            print(f'VAL-LOSS: {np.mean(valid_losses):.4f}',end='\\n')\n",
    "\n",
    "        return epoch_train_losses, epoch_val_losses\n",
    "        \n",
    "    def evaluate(self):\n",
    "\n",
    "        self.model.eval()\n",
    "        times = [0.5, 1, 1.5]\n",
    "        time_to_ind = {0.5:0, 1:1, 1.5:2}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_separate = [[],[],[]]\n",
    "            overall_loss = []\n",
    "            for x,y in self.testing_DataLoader:\n",
    "\n",
    "                x,y = x.float().to(self.device), y.float().to(self.device)\n",
    "                out = self.model(x)\n",
    "\n",
    "                loss = 0\n",
    "                for i in range(3):\n",
    "                    loss_separate[i].append(self.criterion(y[:,i], out[:,i]).item())\n",
    "                    loss+=loss_separate[i][-1]\n",
    "                overall_loss.append(loss/3)\n",
    "\n",
    "        print(f'Error for 0.5 seconds: {np.mean(loss_separate[0]):.4f}; for 1.0 second: {np.mean(loss_separate[1]):.4f}, for 1.5 seconds: {np.mean(loss_separate[2]):.4f}')\n",
    "        print(f'Error over all time (training like): {np.mean(overall_loss):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lCqMj7lwMvQv",
   "metadata": {
    "id": "lCqMj7lwMvQv"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I_Z-Jfm4M1gy",
   "metadata": {
    "id": "I_Z-Jfm4M1gy"
   },
   "source": [
    "### Training with max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2ZFQz0CzM2nE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ZFQz0CzM2nE",
    "outputId": "0c18ed1f-6d41-49ef-a4f5-3a7ecff5c9f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:02<00:50,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 1.7652 VAL-LOSS: 0.8620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:05<00:47,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.7043 VAL-LOSS: 0.6117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:07<00:44,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.5594 VAL-LOSS: 0.5188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:10<00:41,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.4992 VAL-LOSS: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:12<00:38,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.4644 VAL-LOSS: 0.4862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:15<00:36,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.4506 VAL-LOSS: 0.4815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:18<00:33,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.4385 VAL-LOSS: 0.4363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:20<00:31,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.4241 VAL-LOSS: 0.4126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:23<00:28,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.4140 VAL-LOSS: 0.4274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:25<00:25,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.4062 VAL-LOSS: 0.4057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:28<00:23,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.3967 VAL-LOSS: 0.3998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:31<00:20,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.3871 VAL-LOSS: 0.3998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:33<00:18,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.3808 VAL-LOSS: 0.3933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:36<00:15,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.3781 VAL-LOSS: 0.3781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:38<00:13,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.3690 VAL-LOSS: 0.3769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:41<00:10,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.3682 VAL-LOSS: 0.3776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:44<00:07,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.3699 VAL-LOSS: 0.3714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:46<00:05,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.3607 VAL-LOSS: 0.3550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:49<00:02,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.3618 VAL-LOSS: 0.3717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:52<00:00,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.3595 VAL-LOSS: 0.3682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for 0.5 seconds: 0.2818; for 1.0 second: 0.3414, for 1.5 seconds: 0.4955 Error over all time (training like): 0.3729\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "model = ParticleModelLSTM(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer()\n",
    "TrainingProcedure.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-KQsWYrUP6hm",
   "metadata": {
    "id": "-KQsWYrUP6hm"
   },
   "source": [
    "### Training with mean pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T_QraFA7P_G3",
   "metadata": {
    "id": "T_QraFA7P_G3"
   },
   "source": [
    "#### code changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4eP047vsP-d9",
   "metadata": {
    "id": "4eP047vsP-d9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParticleModelLSTM(torch.nn.Module):\n",
    "    def __init__(self, set_size: int = 5, input_size: int = 5, \n",
    "                 fau1_out: int = 16, gamma1_out: int = 32, fau2_out: int = 32, gamma2_out: int = 32, output_size: int = 2,\n",
    "                 hidden_dim: int = 32, horizon_length: int = 3,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        super(ParticleModelLSTM, self).__init__()\n",
    "        \n",
    "        #### Getting Graph Embedding\n",
    "        \n",
    "        # first iteration \n",
    "        # layer 1\n",
    "        # input target (x_t, y_t, c_t, v_t) + neighbour (x_n, y_n, c_n, v_n) + edge (distance(t, n)) \n",
    "        # as the result input for first layer is x + x + 1 = 2x + 1 (8 + 1 in our case)\n",
    "        self.fau_iteration1 = nn.Sequential(nn.Linear(input_size*2 + 1, fau1_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 2\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_1 \n",
    "        self.gamma_iteration1 = nn.Sequential(nn.Linear(fau1_out+input_size, gamma1_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.embedding_size = gamma1_out\n",
    "\n",
    "        # second iteration \n",
    "        # layer 3\n",
    "        # input target (embedding from layer 1) + neigbour (embedding from layer 1) + edge (distance(t, n))\n",
    "        self.fau_iteration2 = nn.Sequential(nn.Linear(gamma1_out*2 + 1, fau2_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 4\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_3 \n",
    "        self.gamma_iteration2 = nn.Sequential(nn.Linear(fau2_out + gamma1_out, gamma2_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        \n",
    "        #self.final_layer = nn.Sequential(nn.Linear(gamma2_out*set_size, output_size*set_size))\n",
    "\n",
    "         # Recurrent Part\n",
    "        self.rnn = nn.LSTM(gamma2_out*set_size, hidden_dim, num_layers = 1, batch_first = True)\n",
    "        \n",
    "        # Fully connected layer after RNN output, to learn more complex relations\n",
    "        self.final = nn.Linear(hidden_dim, output_size*set_size)\n",
    "\n",
    "\n",
    "        # auxiliary variables \n",
    "        self.output_size = gamma2_out\n",
    "        self.device = device\n",
    "        self.prediction_size = output_size\n",
    "        self.horizon_length = horizon_length\n",
    "        self.set_size = set_size\n",
    "\n",
    "    \n",
    "    def forward_iteration1(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[1], self.embedding_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration1(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.mean(axis = 1)\n",
    "\n",
    "            # concatenate \n",
    "            # x of shape (batch_size, 1, num_features+fau_output)\n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            \n",
    "            embeddings[:,i] = self.gamma_iteration1(x).view(particle_set.shape[0], -1)\n",
    "        # for convenicence, swap axes when return\n",
    "        return embeddings.swapaxes(1,2)\n",
    "\n",
    "    def forward_iteration2(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[2], self.output_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration2(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.mean(axis = 1)\n",
    "\n",
    "            # concatenate the neighborhood and \n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            embeddings[:,i] = self.gamma_iteration2(x).view(particle_set.shape[0], -1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, particle_set):\n",
    "        distances = torch.stack([torch.cdist(x_i, x_i) for x_i in particle_set.swapaxes(1,2)], dim=0).to(self.device)\n",
    "        inner_emedding = self.forward_iteration1(particle_set = particle_set, distances = distances).to(self.device)\n",
    "        embedding = self.forward_iteration2(particle_set = inner_emedding, distances = distances)\n",
    "        zeros = torch.zeros((embedding.shape[0], self.horizon_length, embedding.shape[-1]*self.set_size)).to(self.device) \n",
    "        #(batch_size, 1, embedding_dim)\n",
    "        rnn_input = torch.cat([embedding.view((embedding.shape[0], 1, -1)), zeros], dim=1)\n",
    "        # Input: (Batch size, L (sequence length), input_shape)\n",
    "        # rnn_output: (Batch size, L (sequence length), output_shape)\n",
    "        rnn_output, rnn_state = self.rnn(rnn_input)\n",
    "\n",
    "        output = self.final(rnn_output[:,1:,:])\n",
    "\n",
    "        return output.view((output.shape[0], self.horizon_length, self.prediction_size, self.set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s0EbjfQ0QDFs",
   "metadata": {
    "id": "s0EbjfQ0QDFs"
   },
   "source": [
    "#### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "PBzu6uW3P8rA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBzu6uW3P8rA",
    "outputId": "5850936a-d9c6-4f9f-9c42-32b75f9d450e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:03<01:01,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 1.7670 VAL-LOSS: 0.9098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:05<00:51,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.7038 VAL-LOSS: 0.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:08<00:46,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.5671 VAL-LOSS: 0.5297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:11<00:43,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.5052 VAL-LOSS: 0.4985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:13<00:39,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.4710 VAL-LOSS: 0.4729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:16<00:36,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.4531 VAL-LOSS: 0.4712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:18<00:34,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.4402 VAL-LOSS: 0.4487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:21<00:31,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.4242 VAL-LOSS: 0.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:24<00:30,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.4175 VAL-LOSS: 0.4248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:27<00:27,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.4089 VAL-LOSS: 0.4193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:29<00:24,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.3957 VAL-LOSS: 0.4139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:32<00:21,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.3942 VAL-LOSS: 0.4176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:35<00:19,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.3853 VAL-LOSS: 0.4099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:38<00:16,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.3817 VAL-LOSS: 0.3828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:40<00:13,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.3790 VAL-LOSS: 0.3877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:43<00:10,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.3713 VAL-LOSS: 0.3912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:47<00:09,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.3670 VAL-LOSS: 0.3818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:50<00:06,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.3718 VAL-LOSS: 0.3745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:53<00:02,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.3656 VAL-LOSS: 0.3858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:55<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.3634 VAL-LOSS: 0.3787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for 0.5 seconds: 0.2857; for 1.0 second: 0.3545, for 1.5 seconds: 0.5149 Error over all time (training like): 0.3851\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "model = ParticleModelLSTM(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer()\n",
    "TrainingProcedure.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y7mtM7Q4QWPN",
   "metadata": {
    "id": "y7mtM7Q4QWPN"
   },
   "source": [
    "### More LSTM layers (3 layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Aw9xlcRIQZO2",
   "metadata": {
    "id": "Aw9xlcRIQZO2"
   },
   "source": [
    "#### code changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "s4_pQST6QYyl",
   "metadata": {
    "id": "s4_pQST6QYyl"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParticleModelLSTM(torch.nn.Module):\n",
    "    def __init__(self, set_size: int = 5, input_size: int = 5, \n",
    "                 fau1_out: int = 16, gamma1_out: int = 32, fau2_out: int = 32, gamma2_out: int = 32, output_size: int = 2,\n",
    "                 hidden_dim: int = 32, horizon_length: int = 3,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        super(ParticleModelLSTM, self).__init__()\n",
    "        \n",
    "        #### Getting Graph Embedding\n",
    "        \n",
    "        # first iteration \n",
    "        # layer 1\n",
    "        # input target (x_t, y_t, c_t, v_t) + neighbour (x_n, y_n, c_n, v_n) + edge (distance(t, n)) \n",
    "        # as the result input for first layer is x + x + 1 = 2x + 1 (8 + 1 in our case)\n",
    "        self.fau_iteration1 = nn.Sequential(nn.Linear(input_size*2 + 1, fau1_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 2\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_1 \n",
    "        self.gamma_iteration1 = nn.Sequential(nn.Linear(fau1_out+input_size, gamma1_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.embedding_size = gamma1_out\n",
    "\n",
    "        # second iteration \n",
    "        # layer 3\n",
    "        # input target (embedding from layer 1) + neigbour (embedding from layer 1) + edge (distance(t, n))\n",
    "        self.fau_iteration2 = nn.Sequential(nn.Linear(gamma1_out*2 + 1, fau2_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 4\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_3 \n",
    "        self.gamma_iteration2 = nn.Sequential(nn.Linear(fau2_out + gamma1_out, gamma2_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        \n",
    "        #self.final_layer = nn.Sequential(nn.Linear(gamma2_out*set_size, output_size*set_size))\n",
    "\n",
    "         # Recurrent Part\n",
    "        self.rnn = nn.LSTM(gamma2_out*set_size, hidden_dim, num_layers = 3, batch_first = True)\n",
    "        \n",
    "        # Fully connected layer after RNN output, to learn more complex relations\n",
    "        self.final = nn.Linear(hidden_dim, output_size*set_size)\n",
    "\n",
    "\n",
    "        # auxiliary variables \n",
    "        self.output_size = gamma2_out\n",
    "        self.device = device\n",
    "        self.prediction_size = output_size\n",
    "        self.horizon_length = horizon_length\n",
    "        self.set_size = set_size\n",
    "\n",
    "    \n",
    "    def forward_iteration1(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[1], self.embedding_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration1(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate \n",
    "            # x of shape (batch_size, 1, num_features+fau_output)\n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            \n",
    "            embeddings[:,i] = self.gamma_iteration1(x).view(particle_set.shape[0], -1)\n",
    "        # for convenicence, swap axes when return\n",
    "        return embeddings.swapaxes(1,2)\n",
    "\n",
    "    def forward_iteration2(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[2], self.output_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration2(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate the neighborhood and \n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            embeddings[:,i] = self.gamma_iteration2(x).view(particle_set.shape[0], -1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, particle_set):\n",
    "        distances = torch.stack([torch.cdist(x_i, x_i) for x_i in particle_set.swapaxes(1,2)], dim=0).to(self.device)\n",
    "        inner_emedding = self.forward_iteration1(particle_set = particle_set, distances = distances).to(self.device)\n",
    "        embedding = self.forward_iteration2(particle_set = inner_emedding, distances = distances)\n",
    "        zeros = torch.zeros((embedding.shape[0], self.horizon_length, embedding.shape[-1]*self.set_size)).to(self.device) \n",
    "        #(batch_size, 1, embedding_dim)\n",
    "        rnn_input = torch.cat([embedding.view((embedding.shape[0], 1, -1)), zeros], dim=1)\n",
    "        # Input: (Batch size, L (sequence length), input_shape)\n",
    "        # rnn_output: (Batch size, L (sequence length), output_shape)\n",
    "        rnn_output, rnn_state = self.rnn(rnn_input)\n",
    "\n",
    "        output = self.final(rnn_output[:,1:,:])\n",
    "\n",
    "        return output.view((output.shape[0], self.horizon_length, self.prediction_size, self.set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nD_v_TSwQi3k",
   "metadata": {
    "id": "nD_v_TSwQi3k"
   },
   "source": [
    "#### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b-A9_HLsQmhc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-A9_HLsQmhc",
    "outputId": "05a99385-4499-4354-9db6-c9cde1f37383"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:03<01:02,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 2.4154 VAL-LOSS: 1.4600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:06<00:58,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 1.1768 VAL-LOSS: 1.0821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:09<00:54,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.7746 VAL-LOSS: 0.7574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:12<00:50,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.6187 VAL-LOSS: 0.5706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:15<00:47,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.5658 VAL-LOSS: 0.5761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:19<00:44,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.5341 VAL-LOSS: 0.5026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:22<00:41,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.5051 VAL-LOSS: 0.5373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:25<00:37,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.4935 VAL-LOSS: 0.5171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:28<00:34,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.4778 VAL-LOSS: 0.4974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:31<00:31,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.4648 VAL-LOSS: 0.5510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:34<00:28,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.4674 VAL-LOSS: 0.4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:38<00:25,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.4459 VAL-LOSS: 0.4279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:41<00:22,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.4447 VAL-LOSS: 0.4437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:44<00:19,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.4369 VAL-LOSS: 0.4746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:47<00:15,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.4255 VAL-LOSS: 0.4494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:50<00:12,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.4226 VAL-LOSS: 0.4311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:54<00:09,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.4279 VAL-LOSS: 0.4433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:58<00:06,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.4211 VAL-LOSS: 0.4232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [01:01<00:03,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.4219 VAL-LOSS: 0.4370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:04<00:00,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.4153 VAL-LOSS: 0.4147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for 0.5 seconds: 0.3063; for 1.0 second: 0.4012, for 1.5 seconds: 0.5621 Error over all time (training like): 0.4232\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "model = ParticleModelLSTM(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer()\n",
    "TrainingProcedure.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oGXojQo8Q_Qc",
   "metadata": {
    "id": "oGXojQo8Q_Qc"
   },
   "source": [
    "### Adding Embedding Encoding before LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36prjWIeRD4h",
   "metadata": {
    "id": "36prjWIeRD4h"
   },
   "source": [
    "#### code changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "OLyMsRNbQ-wF",
   "metadata": {
    "id": "OLyMsRNbQ-wF"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParticleModelLSTM(torch.nn.Module):\n",
    "    def __init__(self, set_size: int = 5, input_size: int = 5, \n",
    "                 fau1_out: int = 16, gamma1_out: int = 32, fau2_out: int = 32, gamma2_out: int = 32, output_size: int = 2,\n",
    "                 lstm_input: int = 32, hidden_dim: int = 32, horizon_length: int = 3,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        super(ParticleModelLSTM, self).__init__()\n",
    "        \n",
    "        #### Getting Graph Embedding\n",
    "        \n",
    "        # first iteration \n",
    "        # layer 1\n",
    "        # input target (x_t, y_t, c_t, v_t) + neighbour (x_n, y_n, c_n, v_n) + edge (distance(t, n)) \n",
    "        # as the result input for first layer is x + x + 1 = 2x + 1 (8 + 1 in our case)\n",
    "        self.fau_iteration1 = nn.Sequential(nn.Linear(input_size*2 + 1, fau1_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 2\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_1 \n",
    "        self.gamma_iteration1 = nn.Sequential(nn.Linear(fau1_out+input_size, gamma1_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        self.embedding_size = gamma1_out\n",
    "\n",
    "        # second iteration \n",
    "        # layer 3\n",
    "        # input target (embedding from layer 1) + neigbour (embedding from layer 1) + edge (distance(t, n))\n",
    "        self.fau_iteration2 = nn.Sequential(nn.Linear(gamma1_out*2 + 1, fau2_out),\n",
    "                                            nn.LeakyReLU())\n",
    "        # embeddings are calculated\n",
    "\n",
    "        # layer 4\n",
    "        # input (x_target, y_target, c_target, v_target) + embedding from layer_3 \n",
    "        self.gamma_iteration2 = nn.Sequential(nn.Linear(fau2_out + gamma1_out, gamma2_out),\n",
    "                                              nn.LeakyReLU())\n",
    "        \n",
    "        self.final_embedding_layer = nn.Sequential(nn.Linear(gamma2_out*set_size, lstm_input),\n",
    "                                         nn.LeakyReLU())\n",
    "\n",
    "         # Recurrent Part\n",
    "        self.rnn = nn.LSTM(lstm_input, hidden_dim, num_layers = 1, batch_first = True)\n",
    "        \n",
    "        # Fully connected layer after RNN output, to learn more complex relations\n",
    "        self.final = nn.Linear(hidden_dim, output_size*set_size)\n",
    "\n",
    "\n",
    "        # auxiliary variables \n",
    "        self.output_size = gamma2_out\n",
    "        self.device = device\n",
    "        self.prediction_size = output_size\n",
    "        self.horizon_length = horizon_length\n",
    "        self.set_size = set_size\n",
    "        self.lstm_input = lstm_input\n",
    "\n",
    "    \n",
    "    def forward_iteration1(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[1], self.embedding_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration1(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate \n",
    "            # x of shape (batch_size, 1, num_features+fau_output)\n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            \n",
    "            embeddings[:,i] = self.gamma_iteration1(x).view(particle_set.shape[0], -1)\n",
    "        # for convenicence, swap axes when return\n",
    "        return embeddings.swapaxes(1,2)\n",
    "\n",
    "    def forward_iteration2(self, particle_set, distances):\n",
    "        \"\"\"\n",
    "          particle_set - the matrix of shape (batch_size, num_features, set_size)\n",
    "        \"\"\"\n",
    "        # matrix of shape (batch_size, set_size, embedding_size)\n",
    "        embeddings = torch.zeros((particle_set.shape[0], particle_set.shape[2], self.output_size)).to(self.device)\n",
    "        \n",
    "        #iterate over the particles in the set\n",
    "        for i in range(particle_set.shape[-1]):\n",
    "            # concatenate the neighborhood\n",
    "            \n",
    "            # x of shape (batch_size, set_size - 1, num_features*2 + 1)\n",
    "            x = torch.cat([# duplicate the vector of target particle\n",
    "                           particle_set[:,:,i].reshape((particle_set.shape[0], 1, -1)).repeat((1, particle_set.shape[-1] - 1, 1)),\n",
    "                           # choose vectors of all other particles\n",
    "                           particle_set[:,:, list(set(range(particle_set.shape[-1])).difference({i}))].swapaxes(1,2),\n",
    "                           # choose distances from i-th particle to all other particles\n",
    "                           distances[:, i, list(set(range(particle_set.shape[-1])).difference({i}))].reshape((particle_set.shape[0],\\\n",
    "                                                                                                              particle_set.shape[-1]-1,-1))],\n",
    "                          dim=2)\n",
    "\n",
    "            x = self.fau_iteration2(x)\n",
    "\n",
    "            # aggregation function over dimension = 1 -- the whole neighborhood\n",
    "            x = x.max(axis = 1).values\n",
    "\n",
    "            # concatenate the neighborhood and \n",
    "            x = torch.cat([particle_set[:,:, i].view(particle_set.shape[0], 1, -1),\n",
    "                           x.view(particle_set.shape[0], 1, -1)],\n",
    "                           dim = 2)\n",
    "            embeddings[:,i] = self.gamma_iteration2(x).view(particle_set.shape[0], -1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, particle_set):\n",
    "        distances = torch.stack([torch.cdist(x_i, x_i) for x_i in particle_set.swapaxes(1,2)], dim=0).to(self.device)\n",
    "        inner_emedding = self.forward_iteration1(particle_set = particle_set, distances = distances).to(self.device)\n",
    "        embedding = self.forward_iteration2(particle_set = inner_emedding, distances = distances)\n",
    "\n",
    "        embedding = self.final_embedding_layer(embedding.view((embedding.shape[0], 1, -1)))\n",
    "\n",
    "        zeros = torch.zeros((embedding.shape[0], self.horizon_length, embedding.shape[-1])).to(self.device) \n",
    "        #(batch_size, 1, embedding_dim)\n",
    "        rnn_input = torch.cat([embedding.view((embedding.shape[0], 1, -1)), zeros], dim=1)\n",
    "        # Input: (Batch size, L (sequence length), input_shape)\n",
    "        # rnn_output: (Batch size, L (sequence length), output_shape)\n",
    "        rnn_output, rnn_state = self.rnn(rnn_input)\n",
    "\n",
    "        output = self.final(rnn_output[:,1:,:])\n",
    "\n",
    "        return output.view((output.shape[0], self.horizon_length, self.prediction_size, self.set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VK3IfeG7RFTV",
   "metadata": {
    "id": "VK3IfeG7RFTV"
   },
   "source": [
    "#### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "vK3fyTGYRGDL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vK3fyTGYRGDL",
    "outputId": "436423a8-177c-45e8-883a-bc3ad625ec25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:04<01:21,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 01/20 LOSS: 2.0521 VAL-LOSS: 1.0910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:06<01:00,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 02/20 LOSS: 0.8283 VAL-LOSS: 0.6878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:09<00:51,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 03/20 LOSS: 0.6365 VAL-LOSS: 0.6558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:12<00:46,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 04/20 LOSS: 0.5706 VAL-LOSS: 0.5278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:14<00:42,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 05/20 LOSS: 0.5310 VAL-LOSS: 0.5090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:17<00:38,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 06/20 LOSS: 0.5010 VAL-LOSS: 0.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:20<00:35,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 07/20 LOSS: 0.4903 VAL-LOSS: 0.4846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:23<00:32,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 08/20 LOSS: 0.4681 VAL-LOSS: 0.4669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:25<00:29,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 09/20 LOSS: 0.4726 VAL-LOSS: 0.4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:28<00:27,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/20 LOSS: 0.4559 VAL-LOSS: 0.4423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:31<00:24,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11/20 LOSS: 0.4494 VAL-LOSS: 0.4462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:33<00:21,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12/20 LOSS: 0.4440 VAL-LOSS: 0.4716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:36<00:18,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 13/20 LOSS: 0.4508 VAL-LOSS: 0.4472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:39<00:16,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 14/20 LOSS: 0.4266 VAL-LOSS: 0.4403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:41<00:13,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15/20 LOSS: 0.4286 VAL-LOSS: 0.4398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:44<00:10,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 16/20 LOSS: 0.4233 VAL-LOSS: 0.4215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:47<00:08,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 17/20 LOSS: 0.4230 VAL-LOSS: 0.4562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:49<00:05,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 18/20 LOSS: 0.4231 VAL-LOSS: 0.4439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:52<00:02,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 19/20 LOSS: 0.4178 VAL-LOSS: 0.4289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:55<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20/20 LOSS: 0.4099 VAL-LOSS: 0.3971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for 0.5 seconds: 0.2828; for 1.0 second: 0.3716, for 1.5 seconds: 0.5452 Error over all time (training like): 0.3999\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "model = ParticleModelLSTM(device = device).to(device)\n",
    "criterion = ParticleDistanceLoss(norm=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "TrainingProcedure = Trainer(model, \n",
    "                            device, \n",
    "                            criterion, \n",
    "                            optimizer,\n",
    "                            train_dataloader,\n",
    "                            valid_dataloader,\n",
    "                            test_dataloader,\n",
    "                            epochs = 20)\n",
    "\n",
    "train_loss, val_loss = TrainingProcedure.run_trainer()\n",
    "TrainingProcedure.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eiMjFFLbR0CE",
   "metadata": {
    "id": "eiMjFFLbR0CE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "faec2056",
    "059b633c",
    "18b2874d",
    "dea70d73",
    "O3jtlEzZEwTE",
    "b5cf8f64",
    "qt1bd2y9C760",
    "w3kzTcgQHHdg",
    "o09IWs0mKmaA",
    "I_Z-Jfm4M1gy",
    "-KQsWYrUP6hm",
    "T_QraFA7P_G3",
    "s0EbjfQ0QDFs",
    "y7mtM7Q4QWPN",
    "Aw9xlcRIQZO2",
    "36prjWIeRD4h",
    "VK3IfeG7RFTV"
   ],
   "name": "Assignment_2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "b001d610a241339cc3b7988a7f6c804c70cb4dbbf032519cbfea0d67797e8b2b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
