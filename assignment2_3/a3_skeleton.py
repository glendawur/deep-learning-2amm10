# -*- coding: utf-8 -*-
"""a3_skeleton.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13AiLaFUgVTdl3aNusQ5hKkAUa76xv4xp

# Group Number: 32

# Student 1: Ryan Meghoe

# Student 2: Nikita Jain

# Student 3: Andrei Rykov

# Downloading Data and Preliminaries
"""

import pickle
import matplotlib.pyplot as plt
import matplotlib
import numpy as np

from zipfile import ZipFile
import requests
import io

def load_zip(url):
    response = requests.get(url)
    response.raise_for_status()
    zipf = ZipFile(io.BytesIO(response.content))
    return {name: zipf.read(name) for name in zipf.namelist()}

def load_pickle(zipfile, fn):
    return pickle.load(io.BytesIO(zipfile[fn]))

data = load_zip('https://surfdrive.surf.nl/files/index.php/s/cwqGaS22KXgnXtg/download')

    
"""
simulation_{train, valid, test} is stored as a list of simulations. 
Each simulation is a numpy array of size (t, 2): For t timesteps an x and y coordinate of our particle.
"""
simulation_train = load_pickle(data, 'data/train/simulation.pickle')  # 3.1 + 3.2
simulation_valid = load_pickle(data, 'data/valid/simulation.pickle')  # 3.1 + 3.2
simulation_test = load_pickle(data, 'data/test/simulation.pickle')  # 3.1 + 3.2

"""
charges_{train, valid, test} is stored as a list of simulation-charges. 
These charges are stored as numpy arrays of size (3,): One value for each charge.
"""
charges_train = load_pickle(data, 'data/train/charges.pickle')  # 3.1
charges_valid = load_pickle(data, 'data/valid/charges.pickle')  # 3.1
charges_test = load_pickle(data, 'data/test/charges.pickle')  # 3.1

"""
simulation_continued_{train, valid, test} is stored as a list of simulations. 
Each simulation is a numpy array of size (t, 2): For t timesteps an x and y coordinate of our particle.
"""
simulation_continued_train = load_pickle(data, 'data/train/simulation_continued.pickle')  # 3.2
simulation_continued_valid = load_pickle(data, 'data/valid/simulation_continued.pickle')  # 3.2
simulation_continued_test = load_pickle(data, 'data/test/simulation_continued.pickle')  # 3.2

"""
Note that the indices are shared throughout the different lists, e.g., for the 4th training simulation:
simulation_train[3] contains its initial simulation
charges_train[3] contains the charges associated with the simulation
simulation_continued_train[3] contains the continuation of the simulation 
                --> simulation_continued_train[3][0] is the state after simulation_train[3][-1]
"""
pass

print('Overview of no. datapoints:\n')

print('Task 3.1:')
print(f'{len(simulation_train)} train, {len(simulation_valid)} validation, {len(simulation_test)} test simulations')
print(f'{len(charges_train)} train, {len(charges_valid)} validation, {len(charges_test)} test charge pairs')
print()

print('Task 3.2:')
print('Since len(simulation_continued_train) < len(simulation_train), we can only use a subset of initial simulations')
print('We cut simulation_train down to the first 150 samples in simulation_train_task32')
simulation_train_task32 = simulation_train[:150]
print(f'{len(simulation_train_task32)} train, {len(simulation_valid)} validation, {len(simulation_test)} test simulations')
print(f'{len(simulation_continued_train)} train, {len(simulation_continued_valid)} validation, {len(simulation_continued_test)} test continuations')

print(f"""
For task 3.1, use:
{chr(10).join(["simulation_{} + charges_{}".format(t, t) for t in ["train", "valid", "test"]])}

For task 3.2, use:
{chr(10).join(["simulation_{} + simulation_continued_{}".format(*((t[0], t[1]) if isinstance(t, tuple) else (t, t))) for t in [("train_task32", "train"), "valid", "test"]])}
""")

print('Print some shapes:\n')
for i in range(3):
    print('simulation_train[{}].shape:'.format(i), simulation_train[i].shape, '-> (t, 2), (x, y) at every t)')
    print('charges_train[{}].shape:'.format(i), charges_train[i].shape, '-> charges for the simulation')
    print('simulation_continued_train[{}].shape:'.format(i), simulation_continued_train[i].shape, '-> (t, 2), (x, y) at every t)')
    print('----\n')

def plot_example(x, x_gt=None, x_pred=None, fn=None):
    charge_locations = np.array([[-1.53846154, -1.53846154],
                                 [ 1.53846154, -1.53846154],
                                 [ 0.        ,  1.53846154]])  # charge locations are fixed
    fig = plt.figure()
    axes = plt.gca()
    axes.set_xlim([-5., 5.])
    axes.set_ylim([-5., 5.])
    cmap = matplotlib.cm.get_cmap('tab20')
    plt.plot(x[:, 0], x[:, 1], color=cmap(0))
    plt.plot(x[0, 0], x[0, 1], 'd', color=cmap(1))
    fig.set_size_inches(5, 5)
    for charge in charge_locations:
        plt.plot(charge[0], charge[1], 'd', color='black')
    if x_gt is not None:
        plt.plot(x_gt[:, 0], x_gt[:, 1], color='red', linewidth=.5)
    if x_pred is not None:
        plt.plot(x_pred[:, 0], x_pred[:, 1], color='green', linestyle='--')
    if fn is None:
        plt.show()
    else:
        plt.savefig(fn)

test_idx = np.random.randint(150)
plot_example(simulation_train[test_idx], simulation_continued_train[test_idx])
print(f'Charges are {charges_train[test_idx]}')

"""## Data Handling and Preprocessing"""

import torch.nn.functional as f
import torch

simulation_train = torch.cat(list(map(lambda x: f.pad(torch.Tensor(x), pad=(0, 0, 110 - x.shape[0], 0)).unsqueeze(0), simulation_train)))
simulation_valid = torch.cat(list(map(lambda x: f.pad(torch.Tensor(x), pad=(0, 0, 110 - x.shape[0], 0)).unsqueeze(0), simulation_valid)))
simulation_test = torch.cat(list(map(lambda x: f.pad(torch.Tensor(x), pad=(0, 0, 110 - x.shape[0], 0)).unsqueeze(0), simulation_test)))


simulation_continued_train = torch.cat(list(map(lambda x: f.pad(torch.Tensor(x), pad=(0, 0, 0, 60 - x.shape[0])).unsqueeze(0), simulation_continued_train)))
simulation_continued_valid = torch.cat(list(map(lambda x: f.pad(torch.Tensor(x), pad=(0, 0, 0, 60 - x.shape[0])).unsqueeze(0), simulation_continued_valid)))
simulation_continued_test = torch.cat(list(map(lambda x: f.pad(torch.Tensor(x), pad=(0, 0, 0, 60 - x.shape[0])).unsqueeze(0), simulation_continued_test)))


charges_train = torch.cat(list(map(lambda x: torch.Tensor(x).unsqueeze(0), charges_train)))
charges_valid = torch.cat(list(map(lambda x: torch.Tensor(x).unsqueeze(0), charges_valid)))
charges_test = torch.cat(list(map(lambda x: torch.Tensor(x).unsqueeze(0), charges_test)))

from torch.utils.data import TensorDataset, DataLoader

lengths_train = torch.Tensor(list(map(lambda x: x.shape[0], simulation_continued_train))).unsqueeze(-1)
lengths_valid = torch.Tensor(map(lambda x: x.shape[0], simulation_continued_valid)).unsqueeze(-1)
lengths_test = torch.Tensor(map(lambda x: x.shape[0], simulation_continued_test)).unsqueeze(-1)

train_dataset_31 = TensorDataset(simulation_train, charges_train)
valid_dataset_31 = TensorDataset(simulation_valid, charges_valid)
test_dataset_31 = TensorDataset(simulation_test, charges_test)

train_dataset_32 = TensorDataset(simulation_train, simulation_continued_train, lengths_train)
valid_dataset_32 = TensorDataset(simulation_valid, simulation_continued_valid, lengths_valid)
test_dataset_32 = TensorDataset(simulation_test, simulation_continued_test, lengths_test)

batch_size = 64

train_dataloader_31 = DataLoader(train_dataset_31, batch_size=batch_size, shuffle=True)
valid_dataloader_31 = DataLoader(valid_dataset_31, batch_size=batch_size, shuffle=False)
test_dataloader_31 = DataLoader(test_dataset_31, batch_size=batch_size, shuffle=False)

train_dataloader_32 = DataLoader(train_dataset_32, batch_size=batch_size, shuffle=True)
valid_dataloader_32 = DataLoader(valid_dataset_32, batch_size=batch_size, shuffle=False)
test_dataloader_32 = DataLoader(test_dataset_32, batch_size=batch_size, shuffle=False)


"""# Task 3.1

## Model Implementation
"""

import torch
import torch.nn as nn

class Encoder(nn.Module):
    def __init__(self, coord_shape, emb_dim, hid_dim, n_layers, dropout):
        super(Encoder).__init__()
        
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        ### Your code here ###
        #self.embedding = nn.Sequential(nn.Linear(coord_shape, emb_dim),
        #                               nn.Dropout(droupout))
        
        self.rnn = nn.LSTM(coord_shape, hid_dim, n_layers, dropout = dropout)
        
    def forward(self, input):
        
        _, (hidden, cell) = self.rnn(input)

        return hidden, cell

class SequenceToCharge(nn.Module):
    def __init__(self, coord_shape, emb_dim, hid_dim, n_layers, dropout,
                 output_shape):
        super(SequenceToCharge).__init__()

        self.encoder = Encoder(coord_shape, emb_dim, hid_dim, n_layers, dropout)

        self.dense = nn.Sequential(nn.Linear(hid_dim, output_shape)
                                #, nn.ReLU(),
                                #  nn.Linear(hidden, output_shape)  
                                  )

    def forward(self, particle_trajectory):
        hidden, _ = self.encoder(particle_trajectory)
        output = self.dense(hidden[-1])

        return output

"""## Model Training"""

from tqdm import tqdm

class Trainer31():
    def __init__(self,
                 model: torch.nn.Module,
                 device: torch.device,
                 criterion: torch.nn.Module,
                 optimizer: torch.optim.Optimizer,
                 training_DataLoader: torch.utils.data.Dataset,
                 validation_DataLoader: torch.utils.data.Dataset ,
                 testing_DataLoader: torch.utils.data.Dataset ,
                 epochs: int
                 ):
        
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.training_DataLoader = training_DataLoader
        self.validation_DataLoader = validation_DataLoader
        self.testing_DataLoader = testing_DataLoader
        self.device = device
        self.epochs = epochs


    def run_trainer(self):

        epoch_train_losses = []
        epoch_val_losses = []

        for epoch in tqdm(range(self.epochs)):
                 

            self.model.train()  # train mode

            train_losses=[]
            
            correct = 0
            length = 0
            for x, y in self.training_DataLoader:

                sequence, charges = x.float().to(self.device), y.float().to(self.device) # send to device (GPU or CPU)

                self.optimizer.zero_grad()  # zerograd the parameters

                loss = 0
                out = self.model(sequence)  # one forward pass

                loss += self.criterion(out, charges)  # calculate loss
                
                loss_value = loss.item()
                train_losses.append(loss_value)
                 
                loss.backward()  # one backward pass
                self.optimizer.step()  # update the parameters
            
            epoch_train_losses.append(np.mean(train_losses))
            self.model.eval()  # evaluation mode
            valid_losses = []  # accumulate the losses here

            correct = 0
            length = 0
            for x,  y in self.validation_DataLoader:

                sequence, charges = x.float().to(self.device), y.float().to(self.device) # send to device (GPU or CPU)

                with torch.no_grad():
                    loss = 0
                    
                    out = self.model(sequence)  # one forward pass
                    loss += self.criterion(out, charges)  # calculate loss
                 
                    loss_value = loss.item()
                    valid_losses.append(loss_value)

            epoch_val_losses.append(np.mean(valid_losses))
                
            # print the results
            print(
                f'EPOCH: {epoch+1:0>{len(str(self.epochs))}}/{self.epochs}',
                end=' '
            )
            print(f'LOSS: {np.mean(train_losses):.4f}',end=' ')
            print(f'VAL-LOSS: {np.mean(valid_losses):.4f}',end='\n')

        return epoch_train_losses, epoch_val_losses
        
    def evaluate(self, target_time):

        self.model.eval()

        with torch.no_grad():
            loss = []
            length = 0
            for x,y in self.testing_DataLoader:
                
                sequence, charges = x.float().to(self.device), y.float().to(self.device)
                out = self.model(sequence)
                loss.append(self.criterion(out, charges))
                
        print(f'Error: {np.mean(loss):.4f}')





"""## Evaluation"""

#todo





"""# Task 3.2

## Model Implementation
"""

class Decoder(nn.Module):
    def __init__(self, input_shape, hid_dim, n_layers, dropout):
        super(Decoder).__init__()

        self.hid_dim = hid_dim
        self.n_layers = n_layers

        self.rnn = nn.LSTM(input_shape, hid_dim, n_layers, dropout=dropout, batch_first = True)

    def forward(self, input, hidden, cell):
        
        output, (hidden, cell) = self.rnn(input, (hidden, cell))

        return output, (hidden, cell)

import numpy as np 

class Seq2SeqARG(nn.Module):
    def __init__(self, coord_shape, emb_dim, hid_dim, n_layers, dropout, output_shape):
        super(Seq2SeqARG).__init__()

        self.encoder = Encoder(coord_shape, emb_dim, hid_dim, n_layers, dropout)

        self.decoder = Decoder(input_shape=coord_shape, hid_dim=hid_dim, n_layers=n_layers, dropout=dropout)

        self.dense = nn.Sequential(nn.Linear(hid_dim, coord_shape))

    def forward(self, particle_trajectory, target_trajectory, teacher_forcing_ratio = 0.5):
        
        _, (hidden, cell) = self.encoder(particle_trajectory)
        
        predicted_target = torch.zeros((target_trajectory.shape)).to(self.device)

        input = particle_trajectory[:,-1,:]
        for t in range(0, target_trajectory.shape[1]):
            output, hidden, cell = self.decoder(input, hidden, cell)
            # output = (batch_size, 1, hid_dim)

            predicted_target[:,t] = self.dense(output[:, 0, :])

            # ground truth usage
            #decide if we are going to use teacher forcing or not
            teacher_force = np.random.rand(1)[0] > teacher_forcing_ratio
            #get the highest predicted token from our predictions
            top1 = target_trajectory[:,t]

            input = predicted_target[:,t ] if teacher_force else top1
        
        return predicted_target



"""## Model Training"""

#todo

class Trainer32():
    def __init__(self,
                 model: torch.nn.Module,
                 device: torch.device,
                 criterion: torch.nn.Module,
                 optimizer: torch.optim.Optimizer,
                 training_DataLoader: torch.utils.data.Dataset,
                 validation_DataLoader: torch.utils.data.Dataset ,
                 testing_DataLoader: torch.utils.data.Dataset ,
                 epochs: int
                 ):
        
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.training_DataLoader = training_DataLoader
        self.validation_DataLoader = validation_DataLoader
        self.testing_DataLoader = testing_DataLoader
        self.device = device
        self.epochs = epochs


    def run_trainer(self):

        epoch_train_losses = []
        epoch_val_losses = []

        for epoch in tqdm(range(self.epochs)):
                 

            self.model.train()  # train mode

            train_losses=[]
            
            correct = 0
            length = 0
            for x, y, z in self.training_DataLoader:

                x_sequence, y_sequence, lengths = x.float().to(self.device), y.float().to(self.device), z.float().to(self.device) # send to device (GPU or CPU)
                

                self.optimizer.zero_grad()  # zerograd the parameters

                loss = 0
                # out - (batch_size, sequence_length, coordinates)
                out = self.model(x_sequence)  # one forward pass

                #for pred,actual,seq_len in zip(out, y_sequence, lengths):

                #    loss += self.criterion(pred[:seq_len], actual[:seq_len])  # calculate loss

                mask = (torch.arange(60)[None, :] < lengths.view(-1)[:, None]).unsqueeze(-1)
                loss += torch.sum((out*mask - y_sequence).pow(2), dim=[1,2]) / lengths.view(-1)
                
                loss_value = loss.item()
                train_losses.append(loss_value)
                 
                loss.backward()  # one backward pass
                self.optimizer.step()  # update the parameters
            
            epoch_train_losses.append(np.mean(train_losses))
            self.model.eval()  # evaluation mode
            valid_losses = []  # accumulate the losses here

            correct = 0
            length = 0
            for x, y, z in self.validation_DataLoader:

                x_sequence, y_sequence, lengths = x.float().to(self.device), y.float().to(self.device), z.float().to(self.device)

                with torch.no_grad():
                    loss = 0
                    
                    out = self.model(x_sequence)  # one forward pass
                    
                    for pred,actual,seq_len in zip(out, y_sequence, lengths):

                        loss += self.criterion(pred[:seq_len], actual[:seq_len])  # calculate loss
                 
                    loss_value = loss.item()
                    valid_losses.append(loss_value)

            epoch_val_losses.append(np.mean(valid_losses))
                
            # print the results
            print(
                f'EPOCH: {epoch+1:0>{len(str(self.epochs))}}/{self.epochs}',
                end=' '
            )
            print(f'LOSS: {np.mean(train_losses):.4f}',end=' ')
            print(f'VAL-LOSS: {np.mean(valid_losses):.4f}',end='\n')

        return epoch_train_losses, epoch_val_losses
        
    def evaluate(self, target_time):

        self.model.eval()
        times = [0.5, 1, 1.5]
        time_to_ind = {0.5:0, 1:1, 1.5:2}

        with torch.no_grad():
            loss = []
            length = 0
            for x,y,z in self.testing_DataLoader:
                
                x_sequence, y_sequence, lengths = x.float().to(self.device), y.float().to(self.device), z.float().to(self.device)
                out = self.model(x_sequence)
                loss_batch = 0
                for pred,actual,seq_len in zip(out, y_sequence, lengths):
                    loss_batch += self.criterion(pred[:seq_len], actual[:seq_len])  

                loss.append(loss_batch)
        print(f'Error: {np.mean(loss):.4f} for target time {target_time}',end=' ')


"""## Evaluation"""

#todo



